{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Fix relative imports error...\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ..lib import utils\n",
    "# from lib.embeddings import create_embedding_collection\n",
    "from lib import embeddings as emb\n",
    "import utils.arxiv.arxiv_search as axs\n",
    "import numpy as np\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# arx_srch = axs.ArxivScraper()\n",
    "# papers = arx_srch.search_papers(\"Graph Neural Networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_chunks = emb.load_and_chunk_code(\"/home/turbouser44/research/Arxiv2Arxode/sandbox/code_6e04b216-f62b-4afb-93a1-e7fcabea0a8a.py\", single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(code_chunks[0].page_content)\n",
    "code_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embmod = OpenAIEmbeddings()\n",
    "\n",
    "for i,chunk in enumerate(code_chunks):\n",
    "    print(chunk.page_content)\n",
    "    candidate_emb = embmod.embed_documents([chunk.page_content])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_code_chunk = emb.load_and_chunk_code(\"./reference_GAT.py\", single_file=True)\n",
    "ref_emb = embmod.embed_documents([ref_code_chunk[0].page_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_emb = np.array(ref_emb)\n",
    "candidate_emb = np.array(candidate_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# np.dot and cosine similarity are essential equivalent (off by 0.00000001 :p). Meaning the embeddings are normed? Unit length -> np.dot(ref_emb,ref_emb.T) == 1.0\n",
    "print(f\"cos_sim: {cosine_similarity(ref_emb, candidate_emb)}, dotprod: {np.dot(ref_emb, candidate_emb.T)}\")\n",
    "print(f\"diff = {-np.dot(ref_emb, candidate_emb.T) + cosine_similarity(ref_emb, candidate_emb)}\")\n",
    "# The similarity is pretty high most probably due to the fact that the sample graph attention network from keras was first created in 2021 and made it into the models training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bismillah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
