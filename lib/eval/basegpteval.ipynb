{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import signal\n",
    "from filelock import FileLock\n",
    "from itertools import islice\n",
    "import logging\n",
    "\n",
    "# relative import error with python modules and what not, the path needs to change according to cwd :)\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.abspath(\"./\")))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# ray\n",
    "import ray\n",
    "\n",
    "# autogen\n",
    "from autogen import OpenAIWrapper\n",
    "\n",
    "# a2a\n",
    "from agents.agent import CodingAssistant\n",
    "from agents.agent_conf import base_cfg\n",
    "from agents.agent import marl\n",
    "from lib.embeddings import get_db_connection\n",
    "\n",
    "# import lib.eval.MLBench.MLBench_eval as mlb\n",
    "# from agents.coordinator import Coordinator\n",
    "# from agents.agent import marl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140539747501456"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(id(id(id(id))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE\n",
    "tasks_path = \"../../runs/a2a_gpt/indist_python_only.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/turbouser44/miniconda3/envs/a3a/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:151: LangChainPendingDeprecationWarning: This class is pending deprecation and may be removed in a future version. You can swap to using the `PGVector` implementation in `langchain_postgres`. Please read the guidelines in the doc-string of this class to follow prior to migrating as there are some differences between the implementations. See <https://github.com/langchain-ai/langchain-postgres> for details aboutthe new implementation.\n",
      "  warn_deprecated(\n",
      "/home/turbouser44/miniconda3/envs/a3a/lib/python3.11/site-packages/langchain_community/vectorstores/pgvector.py:328: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata.Please note that filtering operators have been changed when using JSOB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create adb migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  warn_deprecated(\n",
      "2024-09-16 18:45:42,349 - autogen.agentchat.contrib.retrieve_user_proxy_agent - WARNING - \u001b[33mdocs_path is not provided in retrieve_config. Will raise ValueError if the collection `autogen-docs` doesn't exist. Set docs_path to None to suppress this warning.\u001b[0m\n",
      "2024-09-16 18:45:43.975070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-16 18:45:43.987991: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-16 18:45:43.991816: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-16 18:45:44.001526: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 18:45:44.654585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/turbouser44/miniconda3/envs/a3a/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "coding_agent = marl(\"MLBench_papers\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intellisense :D\n",
    "coding_agent: CodingAssistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tasks(file_path: str):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            task = json.loads(line)\n",
    "            task_description = task[\"task_description\"]\n",
    "            task_idx = task[\"task_idx\"]\n",
    "\n",
    "            yield {\n",
    "                \"task_description\": task_description,\n",
    "                \"task_idx\": task_idx,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg words/prompt\n",
    "# sum([len(t.split())for t in tasks])/len(tasks)\n",
    "# below is more efficient than sum([i for i in j]) -> [] creates an unnecessary intermediate array\n",
    "# whereas using purely the generator (which is valid because sum expects and iterable) yields the lens for sum to consume one at a time.\n",
    "def avg_words_per_task_description(task_generator):\n",
    "    total_words, task_count = map(sum, zip(*((len(task['task_description'].split()), 1) for task in task_generator)))\n",
    "    return total_words / task_count if task_count > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(load_tasks(tasks_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737.2095238095238"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg_words_per_task_description(load_tasks(tasks_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = load_tasks(tasks_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Handlers.SIG_DFL: 0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise TimeoutException(\"Prompt skipped due to timeout\")\n",
    "\n",
    "\n",
    "signal.signal(signal.SIGALRM, handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_with_gpt2(task_generator):\n",
    "    client = OpenAIWrapper(config_list=base_cfg[\"config_list\"])\n",
    "    dbconn = get_db_connection(\"MLBench_papers\")\n",
    "    all_combined_stats = []\n",
    "    with open(\"baseGPT4full.jsonl\", \"a\") as f:\n",
    "\n",
    "        # The previous attempt failed at task 93, so index 92\n",
    "        # NOTE: ADDED ISLICE\n",
    "        for task_desc in islice(task_generator, 92, None):\n",
    "            vec_db_query = client.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Create a query for the following task:\\n{task_desc['task_description']}\\n\"\n",
    "                        \"The query will be used to search a vector database with relevant research papers in it, keep the query short, useful, and relevant. \"\n",
    "                        \"DO NOT RESPOND WITH ANYTHING BUT A QUERY\",\n",
    "                    }\n",
    "                ],\n",
    "                model=\"gpt-3.5-turbo-0125\",\n",
    "                cache_seed=33,\n",
    "            )\n",
    "            vec_db_query = vec_db_query.choices[0].message.content\n",
    "            assert vec_db_query, \"No query generated for vector database search.\"\n",
    "\n",
    "            retrieved_excerpts = dbconn.similarity_search(vec_db_query)\n",
    "            context = [d.page_content for d in retrieved_excerpts]\n",
    "            formatted_context = \"\".join(\n",
    "                [f\"Context {i + 1}: {doc}\" for i, doc in enumerate(context)]\n",
    "            )\n",
    "\n",
    "            exe_feedback = []\n",
    "            attempts = 0\n",
    "\n",
    "            while attempts < 5:\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": f\"You are tasked with generating code based on research papers. \"\n",
    "                        f\"Think through the process step by step and generate complete working code. \"\n",
    "                        f\"Do not fill in functions with 'pass' or ellipses, and do not make assumptions without justification. \"\n",
    "                        f\"Here is the code generated from your previous attempt:\\n{exe_feedback[-1]['code'] if exe_feedback else ''}\\n\"\n",
    "                        f\"Here is the previous execution log:\\n{exe_feedback[-1]['logs'] if exe_feedback else ''}\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": task_desc[\"task_description\"],\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": f\"Here is retrieved information related to the task:\\n{formatted_context}\",\n",
    "                    },\n",
    "                ]\n",
    "\n",
    "                try:\n",
    "                    signal.alarm(120)\n",
    "                    response = client.create(messages=messages, cache_seed=33)\n",
    "                    generated_content = response.choices[0].message.content\n",
    "                    assert generated_content\n",
    "                    ef = coding_agent.detect_and_execute_code(generated_content)\n",
    "                    exe_feedback.append(*ef)\n",
    "                    attempts += 1\n",
    "                    signal.alarm(0)\n",
    "                except Exception as e:\n",
    "                    exe_feedback.append(\n",
    "                        {\"code\": generated_content, \"exit_code\": -1, \"logs\": str(e)}\n",
    "                    )\n",
    "                    attempts += 1\n",
    "\n",
    "            exit_codes = [feedback[\"exit_code\"] for feedback in exe_feedback]\n",
    "            combined_stats = {\n",
    "                \"task_description\": task_desc[\"task_description\"],\n",
    "                \"exe_feedback\": exe_feedback,\n",
    "                \"task_idx\": task_desc[\"task_idx\"],\n",
    "                \"exit_codes\": exit_codes,\n",
    "            }\n",
    "            all_combined_stats.append(combined_stats)\n",
    "\n",
    "            f.write(json.dumps(combined_stats) + \"\\n\")\n",
    "    return all_combined_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute_with_gpt2(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAME = \"baseGPT_indist_python_only.jsonl\"\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Note: Do not set the signal handler at the global level in the worker processes\n",
    "# We'll set it within the remote function if needed\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def process_task(task_desc):\n",
    "    signal.signal(signal.SIGALRM, handler)\n",
    "    # Recreate client, dbconn, and coding_agent within the function\n",
    "    client = OpenAIWrapper(config_list=base_cfg[\"config_list\"])\n",
    "    dbconn = get_db_connection(\"MLBench_papers\")\n",
    "\n",
    "    coding_agent = marl(\"MLBench_papers\")[-1]  # type: ignore\n",
    "    coding_agent: CodingAssistant\n",
    "\n",
    "    try:\n",
    "        signal.alarm(300)\n",
    "\n",
    "        vec_db_query_response = client.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": (\n",
    "                        f\"Create a query for the following task:\\n{task_desc['task_description']}\\n\"\n",
    "                        \"The query will be used to search a vector database with relevant research papers in it, keep the query short, useful, and relevant. \"\n",
    "                        \"DO NOT RESPOND WITH ANYTHING BUT A QUERY\"\n",
    "                    ),\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            # cache_seed=33, #full\n",
    "            cache_seed=89,  # ID Python ONLY\n",
    "        )\n",
    "        vec_db_query = vec_db_query_response.choices[0].message.content.strip()\n",
    "        assert vec_db_query, \"No query generated for vector database search.\"\n",
    "\n",
    "        # Retrieve context from the vector database\n",
    "        retrieved_excerpts = dbconn.similarity_search(vec_db_query)\n",
    "        context = [d.page_content for d in retrieved_excerpts]\n",
    "        formatted_context = \"\".join(\n",
    "            [f\"Context {i + 1}: {doc}\\n\" for i, doc in enumerate(context)]\n",
    "        )\n",
    "\n",
    "        exe_feedback = []\n",
    "        attempts = 0\n",
    "        generated_content = \"\"\n",
    "\n",
    "        while attempts < 5:\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are tasked with generating code based on research papers. \"\n",
    "                        \"Think through the process step by step and generate complete working code. \"\n",
    "                        \"Do not fill in functions with 'pass' or ellipses, and do not make assumptions without justification. \"\n",
    "                        f\"Here is the code generated from your previous attempt:\\n{exe_feedback[-1]['code'] if exe_feedback else ''}\\n\"\n",
    "                        f\"Here is the previous execution log:\\n{exe_feedback[-1]['logs'] if exe_feedback else ''}\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": task_desc[\"task_description\"],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"Here is retrieved information related to the task:\\n{formatted_context}\",\n",
    "                },\n",
    "            ]\n",
    "\n",
    "            try:\n",
    "                signal.alarm(120)  # Adjust per attempt timeout\n",
    "                response = client.create(messages=messages, cache_seed=33)\n",
    "                generated_content = response.choices[0].message.content.strip()\n",
    "                assert generated_content\n",
    "                # Execute the generated code\n",
    "                ef = coding_agent.detect_and_execute_code(generated_content)\n",
    "                exe_feedback.append(*ef)\n",
    "                signal.alarm(0)\n",
    "            except Exception as e:\n",
    "                exe_feedback.append(\n",
    "                    {\"code\": generated_content, \"exit_code\": -1, \"logs\": str(e)}\n",
    "                )\n",
    "            finally:\n",
    "                attempts += 1\n",
    "                signal.alarm(0)  # Reset the alarm\n",
    "\n",
    "        exit_codes = [feedback[\"exit_code\"] for feedback in exe_feedback]\n",
    "        combined_stats = {\n",
    "            \"task_description\": task_desc[\"task_description\"],\n",
    "            \"exe_feedback\": exe_feedback,\n",
    "            \"task_idx\": task_desc[\"task_idx\"],\n",
    "            \"exit_codes\": exit_codes,\n",
    "        }\n",
    "\n",
    "        # Write the combined_stats to the file using file locking\n",
    "        with FileLock(FNAME + \".lock\"):\n",
    "            with open(FNAME, \"a\") as f:\n",
    "                f.write(json.dumps(combined_stats) + \"\\n\")\n",
    "\n",
    "        logger.info(f\"Task {task_desc['task_idx']} processed successfully.\")\n",
    "\n",
    "    except TimeoutException:\n",
    "        logger.warning(f\"Task {task_desc['task_idx']} timed out.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing task {task_desc['task_idx']}: {e}\")\n",
    "    finally:\n",
    "        signal.alarm(0)  # Ensure the alarm is reset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 18:50:10,568\tINFO worker.py:1774 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "processed_task_ids = set()\n",
    "try:\n",
    "    with open(FNAME, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            processed_task_ids.add(data[\"task_idx\"])\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "tasks_to_process = [\n",
    "    task for task in tasks if task[\"task_idx\"] not in processed_task_ids\n",
    "]\n",
    "\n",
    "futures = [process_task.remote(task) for task in tasks_to_process]\n",
    "\n",
    "ray.get(futures)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = client.messages.create(\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Create a query for the following task:\"\n",
    "                    \"The query will be used to search a vector database with relevant research papers in it, keep the query short, useful, and relevant. \"\n",
    "                    \"DO NOT RESPOND WITH ANYTHING BUT A QUERY\".strip(),\n",
    "                }\n",
    "            ],\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=1000,\n",
    "            # cache_seed=33, #full\n",
    "            # cache_seed=89,  # ID Python ONLY\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"machine learning techniques for natural language processing\"'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = (\n",
    "    \"You are tasked with generating code based on research papers. \"\n",
    "    \"Think through the process step by step and generate complete working code. \"\n",
    "    \"Do not fill in functions with 'pass' or ellipses, and do not make assumptions without justification. \"\n",
    ")\n",
    "messages = [\n",
    "    # {\n",
    "    #     \"role\": \"system\",\n",
    "    #     \"content\": \"You are tasked with generating code based on research papers. \"\n",
    "    #     \"Think through the process step by step and generate complete working code. \"\n",
    "    #     \"Do not fill in functions with 'pass' or ellipses, and do not make assumptions without justification. \"\n",
    "    #     f\"Here is the code generated from your previous attempt:\\n{exe_feedback[-1]['code'] if exe_feedback else ''}\\n\"\n",
    "    #     f\"Here is the previous execution log:\\n{exe_feedback[-1]['logs'] if exe_feedback else ''}\",\n",
    "    # },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"oba saloba\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Here is retrieved information related to the task:\\n{'print hello world '}\".strip(),\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    system=system_message,\n",
    "    temperature=1,\n",
    "    messages=messages,\n",
    "    max_tokens=8192,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in python\n",
      "\n",
      "Here is my step-by-step process and complete working code to accomplish this task:\n",
      "\n",
      "1. We need to use Python's print function to output text to the console.\n",
      "2. The text to print is \"Hello, World!\" (with capitalization and punctuation).\n",
      "3. We'll use a simple print statement with the text as a string argument.\n",
      "\n",
      "Here is the complete Python code:\n",
      "\n",
      "```python\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "This code will print \"Hello, World!\" to the console when run. It uses no assumptions beyond standard Python syntax and doesn't require any additional functions or libraries.\n"
     ]
    }
   ],
   "source": [
    "print(response.content[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a3a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
