{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = read_metrics(\"/home/sonofman/Research/Arxiv2Arxode/lib/eval/stats/cc_a2a_gpt4.json\")\n",
    "# X = pd.DataFrame(x)\n",
    "\n",
    "\n",
    "def load_data_to_dataframe(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    # print(data)\n",
    "    # exit()\n",
    "    flattened_data = []\n",
    "    for task in data:\n",
    "        for attempt in task[\"attempt_results\"]:\n",
    "            for attempt_key, attempt_data in attempt.items():\n",
    "                # print(attempt_data)\n",
    "                # exit()\n",
    "                if (\n",
    "                    attempt_data.get(\"cc_metrics\", {}).get(\n",
    "                        \"avg_cc_over_functions_within_attempt\"\n",
    "                    )\n",
    "                    > 0  # if code is not executable then cyclomatic complexity is -1\n",
    "                ):\n",
    "                    cc_metrics = attempt_data.get(\"cc_metrics\", {}).get(\n",
    "                        \"avg_cc_over_functions_within_attempt\"\n",
    "                    )\n",
    "                else:\n",
    "                    cc_metrics = None\n",
    "\n",
    "                attempt_metrics = {\n",
    "                    \"task_desc\": task[\"task_desc\"],\n",
    "                    \"attempt_key\": attempt_key,\n",
    "                    \"cyclomatic_complexity\": cc_metrics,\n",
    "                    **attempt_data.get(\"halstead_metrics\"),\n",
    "                }\n",
    "\n",
    "                flattened_data.append(attempt_metrics)\n",
    "\n",
    "    return pd.DataFrame(flattened_data)\n",
    "\n",
    "\n",
    "# df = load_data_to_dataframe(\n",
    "# \"/home/sonofman/Research/Arxiv2Arxode/lib/eval/stats/cc_a2a_gpt4.json\"\n",
    "# )\n",
    "\n",
    "# Now you can easily calculate the average cyclomatic complexity for each task\n",
    "# average_cc_per_task = df.groupby(\"task_desc\")[\"average_cc\"].mean().reset_index()\n",
    "\n",
    "# print(average_cc_per_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x['cc']\n",
    "# from radon.metrics import h_visit\n",
    "# analyze_code_complexity(\"import numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Generate random data\\nnp.random.seed(0)\\nn_samples = 100\\nx = np.random.randn(n_samples)\\ny = x * 3 + np.random.randn(n_samples)\\n\\n# Reshape data for sklearn\\nx = x.reshape(-1, 1)\\ny = y.reshape(-1, 1)\\n\\n# Use Linear Regression as the model\\nmodel = LinearRegression()\\nmodel.fit(x, y)\\n\\n# Empirical Risk Minimization\\npredictions = model.predict(x)\\nempirical_risk = np.average((y - predictions) ** 2)\\n\\nprint(\\\"Empirical Risk: \\\",empirical_risk)\")\n",
    "# h_visit(\"import numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Generate random data\\nnp.random.seed(0)\\nn_samples = 100\\nx = np.random.randn(n_samples)\\ny = x * 3 + np.random.randn(n_samples)\\n\\n# Reshape data for sklearn\\nx = x.reshape(-1, 1)\\ny = y.reshape(-1, 1)\\n\\n# Use Linear Regression as the model\\nmodel = LinearRegression()\\nmodel.fit(x, y)\\n\\n# Empirical Risk Minimization\\npredictions = model.predict(x)\\nempirical_risk = np.average((y - predictions) ** 2)\\n\\nprint(\\\"Empirical Risk: \\\",empirical_risk)\")\n",
    "# exec(\"import numpy as np\\nfrom sklearn.linear_model import LinearRegression\\n\\n# Generate random data\\nnp.random.seed(0)\\nn_samples = 100\\nx = np.random.randn(n_samples)\\ny = x * 3 + np.random.randn(n_samples)\\n\\n# Reshape data for sklearn\\nx = x.reshape(-1, 1)\\ny = y.reshape(-1, 1)\\n\\n# Use Linear Regression as the model\\nmodel = LinearRegression()\\nmodel.fit(x, y)\\n\\n# Empirical Risk Minimization\\npredictions = model.predict(x)\\nempirical_risk = np.average((y - predictions) ** 2)\\n\\nprint(\\\"Empirical Risk: \\\",empirical_risk)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2a_gpt4 = load_data_to_dataframe(\n",
    "    \"/home/sonofman/Research/Arxiv2Arxode/lib/eval/stats/code_complexity/cc_a2a_gpt4.json\",\n",
    ")\n",
    "a2a_gpt3 = load_data_to_dataframe(\n",
    "    \"/home/sonofman/Research/Arxiv2Arxode/lib/eval/stats/code_complexity/cc_a2a_turbo.json\",\n",
    ")\n",
    "base_gpt4 = load_data_to_dataframe(\n",
    "    \"/home/sonofman/Research/Arxiv2Arxode/lib/eval/stats/code_complexity/cc_baseGPT.json\",\n",
    ")\n",
    "base_gpt3 = load_data_to_dataframe(\n",
    "    \"/home/sonofman/Research/Arxiv2Arxode/lib/eval/stats/code_complexity/cc_base_turbo.json\",\n",
    ")\n",
    "\n",
    "# df_list = [a2a_gpt4, a2a_gpt3, base_gpt4, base_gpt3]\n",
    "# for df in df_list:\n",
    "#     df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a2a_gpt4['task_desc'].unique()\n",
    "task_desc_order = [\n",
    "    \"Create a python file to help me understand why empirical risk minimization is NP hard.\",\n",
    "    \"Implement the polyak stepsize for the gradient descent algorithm and implement for me in a novel use case to improve my understanding.\",\n",
    "    'Implement mirrored descent for me in a self-contained python file, so I can get a better understanding of it, additionally make it online so that it is \"online mirrored descent\".',\n",
    "    'Show me a meaningul implementation of the \"shampoo\" optimization technique in Python, perhaps pull a model from hugging face to try out your Shampoo implementation on.',\n",
    "    \"Create a python file that implements the main ideas present in the REMEMBER paper, apply it with a minimal experiment on some dataset showing the experience memory technique being applied. Remember you also have access to source code embeddings as well related ot the research paper.\",\n",
    "    \"Create a python file that highlights how exactly experience memory can be updated using a RL policy, recreate a minimal executable example for me, do not make any assumptions or fill any functions with the pass keyword or ellipses.\",\n",
    "    \"Explore the paper that talks about enhancing an RL agents capabilities by way of interacting with an LLM, highlight the main ideas for me and if possible generate a python file with a minimal conceptual recreation that is executable and can produce meaningful output for me.\",\n",
    "    \"Recreate the When2Ask Algorithm for me in a minimal executable python file, highlighting the main techniques the paper introduces. Use any libraries necessary to import language models and set up an environment for testing. Make no assumptions and ensure that the python file is executable and produces output.\",\n",
    "    \"Show me an exmaple of an LLM posed as a planner according the Planner-Actor-Mediator framework. Make no assumptions and ensure that the python file is executable and produces output, do not put the pass keyword or ellipses in any function definitions\",\n",
    "    \"Show me how an RL agents exploration can be guiding with LLM priors according to the paper. Create a minimal example in a self-contained python file that must be executable and produce an output, do not make any assumptions or fill any functions with the pass keyword or ellipses.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    \"#001a3b\",\n",
    "    \"#3266a8\",\n",
    "    \"#ff1500\",\n",
    "    \"#fc9b92\",\n",
    "]\n",
    "\n",
    "\n",
    "colors2 = [\"#EEEEEE\", \"#76ABAE\", \"#222831\", \"#31363F\"]\n",
    "\n",
    "colors3 = [\"#78A083\", \"#50727B\", \"#344955\", \"#35374B\"]\n",
    "\n",
    "colors4 = [\"#FDA403\", \"#E8751A\", \"#898121\", \"#E5C287\"]\n",
    "\n",
    "\n",
    "def plot_metric_comparison_boxplot(\n",
    "    df_list, model_labels, metric=\"cyclomatic_complexity\"\n",
    "):\n",
    "    metric_data = [df[metric] for df in df_list]\n",
    "\n",
    "    means = [df[metric].mean() for df in df_list]  # Calculate mean for each model\n",
    "\n",
    "    customized_labels = [\n",
    "        f\"{label}\\nMean: {mean:.2f}\" for label, mean in zip(model_labels, means)\n",
    "    ]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot(metric_data, labels=customized_labels)\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f\"Comparison of {metric} across Models\")\n",
    "    # plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metric_comparison_swarm(df_list, model_labels, metric=\"cyclomatic_complexity\"):\n",
    "    \"\"\"Plots a comparison of a specified metric across different models.\"\"\"\n",
    "\n",
    "    # Create a new DataFrame for all models\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for df, label in zip(df_list, model_labels):\n",
    "        temp_df = df.copy()\n",
    "        temp_df[\"model\"] = label\n",
    "        all_data = pd.concat([all_data, temp_df])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.swarmplot(x=\"model\", y=metric, data=all_data)\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f\"Comparison of {metric} across Models\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metric_comparison_hist(df_list, model_labels, metric=\"cyclomatic_complexity\"):\n",
    "    \"\"\"Plots a comparison of a specified metric across different models.\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for df, label in zip(df_list, model_labels):\n",
    "        plt.hist(df[metric], bins=30, alpha=0.5, label=label)\n",
    "\n",
    "    plt.xlabel(metric)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Histogram of {metric} across Models\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def statistical_test(df1, df2, metric):\n",
    "    stat, p = ttest_ind(df1[metric], df2[metric], nan_policy=\"omit\")\n",
    "    print(f\"Statistics={stat}, p={p}\")\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print(\"Same distributions (fail to reject H0)\")\n",
    "    else:\n",
    "        print(\"Different distributions (reject H0)\")\n",
    "\n",
    "\n",
    "def plot_metric_comparison_ridgeline(\n",
    "    df_list, model_labels, metric=\"cyclomatic_complexity\"\n",
    "):\n",
    "    \"\"\"Plots a comparison of a specified metric across different models.\"\"\"\n",
    "\n",
    "    # Create a new DataFrame for all models\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for df, label in zip(df_list, model_labels):\n",
    "        temp_df = df.copy()\n",
    "        temp_df[\"model\"] = label\n",
    "        all_data = pd.concat([all_data, temp_df])\n",
    "\n",
    "    means = all_data.groupby(\"model\")[metric].mean()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, model in enumerate(model_labels):\n",
    "        mean_metric = means[model]\n",
    "        label_with_mean = f\"{model} (mean {metric}: {mean_metric:.2f})\"\n",
    "\n",
    "        sns.kdeplot(\n",
    "            data=all_data[all_data[\"model\"] == model][metric],\n",
    "            fill=True,\n",
    "            label=label_with_mean,\n",
    "            alpha=0.4,\n",
    "            levels=40,\n",
    "            color=colors[i],\n",
    "            # bw_adjust=4,\n",
    "            # multiple=\"stack\",\n",
    "        )\n",
    "\n",
    "    plt.xlabel(metric.capitalize())\n",
    "    plt.title(f\"Ridgeline plot of {metric} across Models\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metric_comparison_violin(df_list, model_labels, metric, scale=\"area\"):\n",
    "    all_data = pd.DataFrame()\n",
    "    for df, label in zip(df_list, model_labels):\n",
    "        temp_df = df.copy()\n",
    "        temp_df[\"model\"] = label\n",
    "        all_data = pd.concat([all_data, temp_df])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.violinplot(x=\"model\", y=metric, data=all_data, scale=scale)\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f\"Comparison of {metric} across Models\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_metric_histogram_with_dynamic_binning(df, metric=\"cyclomatic_complexity\"):\n",
    "    data = df[metric]\n",
    "    bins = int(np.sqrt(len(data)))\n",
    "    plt.hist(data, bins=bins, alpha=0.7)\n",
    "    plt.xlabel(metric)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Histogram of {metric} with Dynamic Binning\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_stacked_bar_chart(df_list, model_labels, metric=\"cyclomatic_complexity\"):\n",
    "    combined_data = pd.DataFrame()\n",
    "    for df, label in zip(df_list, model_labels):\n",
    "        temp_df = df.copy()\n",
    "        temp_df[\"model\"] = label\n",
    "        combined_data = pd.concat([combined_data, temp_df], ignore_index=True)\n",
    "        print(df[\"task_desc\"].tolist())\n",
    "\n",
    "    agg_data = combined_data.groupby([\"task_desc\", \"model\"])[metric].mean().unstack()\n",
    "\n",
    "    ax = agg_data.plot(\n",
    "        kind=\"bar\",\n",
    "        stacked=True,\n",
    "        figsize=(12, 6),\n",
    "        colormap=mcolors.ListedColormap(colors),\n",
    "    )\n",
    "    # print(agg_data)\n",
    "\n",
    "    # Corrected to match the number of x-ticks\n",
    "    task_labels = [f\"Task {i+1}\" for i in range(len(agg_data.index))]\n",
    "    ax.set_xticks(range(len(agg_data.index)))  # Ensure we have correct number of ticks\n",
    "    ax.set_xticklabels(task_labels, rotation=45)\n",
    "\n",
    "    plt.xlabel(\"Task\")\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.title(f\"{metric.capitalize()} Comparison Across Models\")\n",
    "    plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2a_gpt4.groupby(\"task_desc\")[\"volume\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_stacked_bar_chart(\n",
    "#     df_list,\n",
    "#     [\"A2A GPT-4\", \"A2A GPT3.5\", \"Base GPT-4\", \"Base GPT-3.5\"],\n",
    "#     \"cyclomatic_complexity\",\n",
    "# )\n",
    "# plot_metric_histogram_with_dynamic_binning(a2a_gpt3)\n",
    "\n",
    "plot_metric_comparison_ridgeline(\n",
    "    [\n",
    "        a2a_gpt4,\n",
    "        a2a_gpt3,\n",
    "        base_gpt4,\n",
    "        base_gpt3,\n",
    "    ],\n",
    "    [\n",
    "        \"A2A GPT-4\",\n",
    "        \"A2A GPT3.5\",\n",
    "        \"Base GPT-4\",\n",
    "        \"Base GPT-3.5\",\n",
    "    ],\n",
    "    metric=\"cyclomatic_complexity\",\n",
    "    # scale=\"width\",\n",
    ")\n",
    "\n",
    "# # Example statistical test between A2A GPT-4 and No FW GPT-3.5 for cyclomatic complexity\n",
    "# statistical_test(a2a_gpt4, base_gpt4, \"cyclomatic_complexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a2a_gpt3[\"time\"].dropna()))\n",
    "sum(a2a_gpt3[\"time\"].dropna()) / len(a2a_gpt3[\"time\"].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2a_gpt4[\"time\"].mean(), a2a_gpt3[\"time\"].mean(), base_gpt4[\n",
    "    \"time\"\n",
    "].mean(), base_gpt3[\"time\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_gpt4.groupby(\"task_desc\")[\"cyclomatic_complexity\"].mean().reset_index()\n",
    "# a2a_gpt4.groupby(\"task_desc\")[\"time\"].mean().reset_index()[\"time\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis import calculate_success_rates_from_combined_stats\n",
    "\n",
    "with open(\n",
    "    \"./stats/tasks_and_code_exe_results/combined_stats_baseGPT.jsonl\", \"r\"\n",
    ") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "x = calculate_success_rates_from_combined_stats(data)\n",
    "print(x)\n",
    "\n",
    "numeric_cols = [\n",
    "    \"cyclomatic_complexity\",\n",
    "    \"volume\",\n",
    "    # \"vocabulary\",\n",
    "    # \"difficulty\",\n",
    "    # \"effort\",\n",
    "    \"time\",\n",
    "]\n",
    "metric = numeric_cols[0]\n",
    "success_rates_list = []\n",
    "\n",
    "for _, rate in x.items():\n",
    "    success_rates_list.append(rate)\n",
    "# print(success_rates_list)\n",
    "\n",
    "\n",
    "# df_avg = base_gpt3.copy()\n",
    "df_avg = base_gpt4.groupby(\"task_desc\", as_index=False)[numeric_cols].mean()\n",
    "\n",
    "\n",
    "df_avg = df_avg.set_index(\"task_desc\").loc[task_desc_order].reset_index()\n",
    "\n",
    "# it is important to assign the success rates list which is in the desired order of tasks, after the groupby operation and ordering operation above.\n",
    "df_avg[\"success_rate\"] = success_rates_list\n",
    "\n",
    "# For models that have nan values, fill with 0 or mean of metric..., dropna is what was done for plots presented in the paper. A2AGPT4 has no NaNs.\n",
    "# df_avg.fillna(0, inplace=True)\n",
    "# df_avg.dropna(inplace=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(22, 6))\n",
    "for i, metric in enumerate(numeric_cols):\n",
    "    correlation = np.corrcoef(df_avg[metric], df_avg[\"success_rate\"])\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    sns.regplot(x=metric, y=\"success_rate\", data=df_avg, color=colors2[3])\n",
    "    plt.title(f\"Success Rate vs. {metric.capitalize()}: Base GPT-3.5\")\n",
    "    plt.xlabel(f\"{metric.capitalize()}\")\n",
    "    plt.ylabel(\"Success Rate (%)\")\n",
    "    plt.grid()\n",
    "    plt.legend(\n",
    "        [\n",
    "            \"Correlation: {:.4f}\".format(correlation[0, 1]),\n",
    "        ],\n",
    "        markerfirst=False,\n",
    "    )\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_avg[[\"cyclomatic_complexity\", \"vocabulary\", \"time\", \"success_rate\"]]\n",
    "# pd.set_option(\"display.float_format\", '{:.1f}'.format)\n",
    "# print(df_avg.to_latex(index=False))\n",
    "# selected_columns = [\n",
    "#     \"task_desc\",\n",
    "#     \"cyclomatic_complexity\",\n",
    "#     \"vocabulary\",\n",
    "#     \"time\",\n",
    "#     \"success_rate\",\n",
    "# ]\n",
    "# formatters = {column: \"{:.2f}\".format for column in selected_columns}\n",
    "# print(df_avg[].to_latex(index=False, formatters=formatters))\n",
    "df_avg[\"success_rate\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns\n",
    "numeric_columns = df_avg.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Apply formatter to numeric columns\n",
    "formatters = {column: \"{:.2f}\".format for column in numeric_columns}\n",
    "\n",
    "# Print DataFrame to LaTeX\n",
    "print(df_avg.to_latex(index=False, formatters=formatters, na_rep=\"n/a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_latex_table(data, metrics):\n",
    "    # Start of the LaTeX table\n",
    "    latex_table = \"\\\\begin{table}[H]\\n\\\\centering\\n\"\n",
    "    latex_table += \"\\\\begin{tabular}{|c|c|c|c|}\\n\\\\hline\\n\"\n",
    "    # Table header\n",
    "    header = \"Task Description & \" + \" & \".join(metrics) + \" \\\\\\\\\\\\hline\\n\"\n",
    "    latex_table += header\n",
    "\n",
    "    # Iterate over the JSON data and extract the necessary details\n",
    "    for task in data:\n",
    "        task_desc = task[\"task_desc\"]\n",
    "        for result in task[\"results\"]:\n",
    "            # Starting a new row for each model result\n",
    "            latex_table += f\"{task_desc}\"\n",
    "            for metric in metrics:\n",
    "                if metric == \"avg_cc\":\n",
    "                    value = result[\"cc_metrics\"].get(\n",
    "                        \"avg_cc_over_functions_within_attempt\", 0\n",
    "                    )\n",
    "                else:\n",
    "                    value = result[\"halstead_metrics\"].get(metric, 0)\n",
    "                latex_table += f\" & {value}\"\n",
    "            latex_table += \" \\\\\\\\\\\\hline\\n\"\n",
    "\n",
    "    # End of the LaTeX table\n",
    "    latex_table += \"\\\\end{tabular}\\n\\\\caption{Code Metrics Across Models}\\n\\\\label{tab:metrics}\\n\\\\end{table}\\n\"\n",
    "    return latex_table\n",
    "\n",
    "\n",
    "# Read the JSON data from the file\n",
    "with open(\"./stats/final.json\", \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Generate the LaTeX code for the tables\n",
    "latex_tables = create_latex_table(json_data, [\"avg_cc_over_functions_within_attempt\", \"time\",\"vocabulary\"])\n",
    "\n",
    "# Print the LaTeX code\n",
    "print(latex_tables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bismillah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
