{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from autogen import OpenAIWrapper\n",
    "\n",
    "\n",
    "def load_and_structure_data(jsonl_file_paths):\n",
    "    structured_data = {}\n",
    "    model_map = {\n",
    "        \"combined_stats_a2a.jsonl\": \"a2a_gpt4\",\n",
    "        \"combined_stats_a2a_turbo.jsonl\": \"a2a_turbo\",\n",
    "        \"combined_stats_baseGPT.jsonl\": \"base_gpt4\",\n",
    "        \"combined_stats_base_turbo.jsonl\": \"base_turbo\",\n",
    "    }\n",
    "\n",
    "    for file_path in jsonl_file_paths:\n",
    "        model = model_map[os.path.basename(file_path)]\n",
    "\n",
    "        with open(file_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                data = json.loads(line)\n",
    "                task_desc = data[\"task_description\"]\n",
    "                if task_desc not in structured_data:\n",
    "                    structured_data[task_desc] = {\"attempts\": []}\n",
    "\n",
    "                for feedback in data[\"exe_feedback\"]:\n",
    "                    code = feedback.get(\"code\")\n",
    "                    exit_code = feedback.get(\"exit_code\")\n",
    "\n",
    "                    structured_data[task_desc][\"attempts\"].append(\n",
    "                        {\n",
    "                            \"model\": model,\n",
    "                            \"code\": code,\n",
    "                            \"exit_code\": exit_code,\n",
    "                        }\n",
    "                    )\n",
    "    return structured_data\n",
    "\n",
    "\n",
    "# jsonl_file_paths = [\"path_to_your_first_jsonl_file.jsonl\", \"path_to_your_second_jsonl_file.jsonl\"]\n",
    "# structured_data = load_and_structure_data(jsonl_file_paths)\n",
    "\n",
    "\n",
    "file_paths = [\n",
    "    \"./stats/tasks_and_code_exe_results/combined_stats_a2a.jsonl\",\n",
    "    \"./stats/tasks_and_code_exe_results/combined_stats_a2a_turbo.jsonl\",\n",
    "    \"./stats/tasks_and_code_exe_results/combined_stats_baseGPT.jsonl\",\n",
    "    \"./stats/tasks_and_code_exe_results/combined_stats_base_turbo.jsonl\",\n",
    "]\n",
    "\n",
    "\n",
    "x = load_and_structure_data(file_paths)\n",
    "# _, codes = [get_code_attempts(path) for path in file_paths[1:]]\n",
    "# print(len(x[0]))\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x['Create a python file to help me understand why empirical risk minimization is NP hard.']\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"api_key\": os.environ.get(\"OPENAI_APIKEY2\"),\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4-0125-preview\",\n",
    "        \"api_key\": os.environ.get(\"OPENAI_APIKEY2\"),\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"gpt-4-1106-preview\",\n",
    "        \"api_key\": os.environ.get(\"OPENAI_APIKEY2\"),\n",
    "    },\n",
    "    # {\n",
    "    #     \"model\": \"gpt-3.5-turbo-0125\",\n",
    "    #     \"api_key\": os.environ.get(\"OPENAI_APIKEY2\"),\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model\": \"gpt-3.5-turbo\",\n",
    "    #     \"api_key\": os.environ.get(\"OPENAI_APIKEY2\"),\n",
    "    # },\n",
    "    # {\n",
    "    #     \"model\": \"gpt-3.5-turbo-16k\",\n",
    "    #     \"api_key\": os.environ.get(\"OPENAI_APIKEY2\"),\n",
    "    # },\n",
    "]\n",
    "\n",
    "base_cfg = {\n",
    "    # \"use_cache\": False,\n",
    "    # \"seed\": 22,\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x['Create a python file to help me understand why empirical risk minimization is NP hard.']['attempts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "client = OpenAIWrapper(config_list=base_cfg[\"config_list\"])\n",
    "\n",
    "\n",
    "def semantic_code_analysis(task_description, code):\n",
    "    response = client.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are tasked with providing a review of the over all robustness and quality of the generated code. Use criteria such as correctness, complexity, and MOST IMPORTANTLY adherence to the desire of the task description from the user. Ensure you provide concise step by step reasoning, and to do your best, this is important. After providing a brief analysis of the code quality I want you to give it a rating from 1-10 as an evaluation score in the following way:\\n<Your REVIEW HERE>\\n<Your SCORE (between 1-10) HERE, for example 5>\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Given the task description: {task_description}\\nThe given code:\\n{code}\\n Please provide a review of the code quality and a score based on the given task.\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def integrate_semantic_analysis_with_data(structured_data):\n",
    "\n",
    "    for task_desc, task_data in structured_data.items():\n",
    "        if (\n",
    "            task_desc\n",
    "            == \"Create a python file to help me understand why empirical risk minimization is NP hard.\"\n",
    "        ):\n",
    "            continue\n",
    "        for attempt in task_data[\"attempts\"]:\n",
    "            code = attempt[\"code\"]\n",
    "            # model = attempt[\"model\"]\n",
    "\n",
    "            review = semantic_code_analysis(task_desc, code)\n",
    "\n",
    "            attempt[\"code_review\"] = review\n",
    "        print(task_desc)\n",
    "        # break\n",
    "    return structured_data\n",
    "\n",
    "\n",
    "structured_data_with_reviews = integrate_semantic_analysis_with_data(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".//stats/gpt_code_review/t.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        structured_data_with_reviews,\n",
    "        f,\n",
    "        indent=4,\n",
    "    )\n",
    "# structured_data_with_reviews[\n",
    "#     \"Create a python file to help me understand why empirical risk minimization is NP hard.\"\n",
    "# ]\n",
    "# structured_data_with_reviews['Create a python file to help me understand why empirical risk minimization is NP hard.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(structured_data_with_reviews['Create a python file to help me understand why empirical risk minimization is NP hard.']['attempts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_code_reviews(structured_data):\n",
    "    total_reviews = 0\n",
    "    tasks_without_review = []\n",
    "\n",
    "    for task_desc, task_data in structured_data.items():\n",
    "        for attempt in task_data[\"attempts\"]:\n",
    "            if \"code_review\" not in attempt:\n",
    "                tasks_without_review.append(task_desc)\n",
    "            else:\n",
    "                total_reviews += 1\n",
    "\n",
    "    return total_reviews, tasks_without_review\n",
    "\n",
    "\n",
    "# structured_data_with_reviews = integrate_semantic_analysis_with_data(x)\n",
    "total_reviews, tasks_without_review = verify_code_reviews(structured_data_with_reviews)\n",
    "\n",
    "print(f\"Total code reviews: {total_reviews}\")\n",
    "if tasks_without_review:\n",
    "    print(f\"Tasks without code review: {tasks_without_review}\")\n",
    "else:\n",
    "    print(\"All attempts have code reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(structured_data):\n",
    "    latex_code = (\n",
    "        \"\\\\begin{table}[H]\\n\\\\centering\\n\\\\begin{tabular}{|l|l|l|p{5cm}|}\\n\\\\hline\\n\"\n",
    "    )\n",
    "    latex_code += \"Model & Task & Code (Truncated) & Review (Truncated) \\\\\\\\\\\\hline\\n\"\n",
    "\n",
    "    for task_desc, task_data in structured_data.items():\n",
    "        for attempt in task_data[\"attempts\"]:\n",
    "            model = attempt[\"model\"]\n",
    "            code = attempt[\"code\"][:100] + \"...\"  # Truncate code for brevity\n",
    "            review = (\n",
    "                attempt[\"code_review\"].split(\"\\n\")[0] + \"...\"\n",
    "            )  # Truncate review for brevity\n",
    "            latex_code += f\"{model} & \\\\parbox[t]{{5cm}}{{{task_desc[:50]}...}} & {code} & {review} \\\\\\\\\\\\hline\\n\"\n",
    "\n",
    "    latex_code += \"\\\\end{tabular}\\n\\\\caption{Summary of Code Reviews}\\n\\\\label{tab:code_reviews}\\n\\\\end{table}\"\n",
    "    return latex_code\n",
    "\n",
    "\n",
    "with open(\"./stats/gpt_code_review/t.json\", \"r\") as f:\n",
    "    x = json.load(f)\n",
    "# latex_code = generate_latex_table(x)\n",
    "\n",
    "# Print or save the latex_code to a file\n",
    "# print(latex_code)\n",
    "# with open('code_reviews_table.tex', 'w') as f:\n",
    "#     f.write(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code initially generated by GPT-4 :)\n",
    "\n",
    "def generate_latex_table_and_appendix_single_attempt_per_model(structured_data):\n",
    "    appendix_section = \"\\n\\\\begin{appendix}\\n\"\n",
    "    appendix_section += (\n",
    "        \"    \\\\section{Appendix: Detailed Code Reviews and Generated Code}\\n\"\n",
    "    )\n",
    "\n",
    "    table_code = (\n",
    "        \"\\\\begin{table}[H]\\n\\\\centering\\n\"\n",
    "        \"\\\\begin{tabular}{|l|l|p{6cm}|}\\n\\\\hline\\n\"\n",
    "        \"Model & Task Description & Code and Review Reference \\\\\\\\\\\\hline\\n\"\n",
    "    )\n",
    "\n",
    "    task_counter = 1\n",
    "    for task_desc, task_data in structured_data.items():\n",
    "        appendix_section += f\"    \\\\subsection{{Task {task_counter}: {task_desc}}}\\n\"\n",
    "        appendix_section += f\"    \\\\label{{sec:code{task_counter}}}\\n\\n\"\n",
    "\n",
    "        processed_models = set()  # Track which models have been processed for this task\n",
    "\n",
    "        for attempt in task_data[\"attempts\"]:\n",
    "            model = attempt[\"model\"]\n",
    "\n",
    "            # Skip if this model's attempt has already been processed for this task\n",
    "            if model in processed_models:\n",
    "                continue\n",
    "\n",
    "            processed_models.add(model)  # Mark this model as processed for this task\n",
    "\n",
    "            code = attempt[\"code\"].replace(\"\\n\", \"\\n    \")  # Format code for LaTeX\n",
    "            review = attempt[\"code_review\"].replace(\n",
    "                \"\\n\", \"\\n    \"\n",
    "            )  # Format review for LaTeX\n",
    "\n",
    "            # Add to table and appendix\n",
    "            table_code += f\"{model} & {task_desc[:50]}... & See Appendix, Section \\\\ref{{sec:code{task_counter}-{model}}} \\\\\\\\\\\\hline\\n\"\n",
    "            appendix_section += f\"    \\\\subsubsection{{Model: {model}}}\\n\"\n",
    "            appendix_section += f\"    \\\\label{{sec:code{task_counter}-{model}}}\\n\\n\"\n",
    "            appendix_section += (\n",
    "                \"    \\\\textbf{Generated Code:}\\n\\n    \\\\begin{verbatim}\\n\"\n",
    "            )\n",
    "            appendix_section += f\"    {code}\\n \" + \"\\\\end{verbatim}\\n\\n\"\n",
    "            appendix_section += \"    \\\\textbf{Review:}\\n\\n\" + f\"{review}\\n\\n\"\n",
    "\n",
    "        task_counter += 1\n",
    "\n",
    "    table_code += \"\\\\end{tabular}\\n\\\\caption{Summary of Code Reviews and Generated Code}\\n\\\\label{tab:code-reviews}\\n\\\\end{table}\\n\"\n",
    "    appendix_section += \"\\\\end{appendix}\\n\"\n",
    "\n",
    "    return table_code, appendix_section\n",
    "\n",
    "\n",
    "# Assuming `x` is your structured data\n",
    "latex_table_code, latex_appendix_section = (\n",
    "    generate_latex_table_and_appendix_single_attempt_per_model(x)\n",
    ")\n",
    "\n",
    "# Print or save to a file\n",
    "# print(latex_table_code)\n",
    "print(latex_appendix_section)\n",
    "# You can write these to a .tex file as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table_code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bismillah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
