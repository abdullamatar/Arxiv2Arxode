{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# relative import error with python modules and what not, the path needs to change according to cwd :)\n",
    "# project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "# if project_root not in sys.path:\n",
    "#     sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook extracts GitHub URLS from the [MLBench](https://huggingface.co/datasets/super-dainiu/ml-bench) dataset, searches for any Arxiv links, and then downloads all the papers it has found (exactly 17 of the 18 repositories in the dataset contained links to Arxiv). It then creates one large embeddings database with all the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# a2a\n",
    "# from arxiv2arxode.utils.arxiv import arxiv_search\n",
    "# import arxiv2arxode.lib.embeddings as emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(\"./datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds['quarter'])+ len(ds['full']) + 315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([d for d in ds['id_train'] if d['type']==\"Python Code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['quarter'][1]['github']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_repos = set()\n",
    "for subset in ds.values():\n",
    "    unique_repos.update(subset['github'])\n",
    "unique_repos, len(unique_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GITHUB_API = \"https://api.github.com/repos/{}/readme\"\n",
    "\n",
    "ARXIV_REGEX = re.compile(r'arxiv.org/(abs|pdf)/\\d{4}\\.\\d{5}')\n",
    "\n",
    "def fetch_readme(repo_full_name, token=None):\n",
    "    headers = {\n",
    "        \"X-GitHub-Api-Version\": \"2022-11-28\"\n",
    "    }\n",
    "    if token:\n",
    "        headers['Authorization'] = f'Bearer {token}'\n",
    "\n",
    "    url = GITHUB_API.format(repo_full_name)\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        readme_data = response.json()\n",
    "        readme_content = requests.get(readme_data['download_url']).text\n",
    "        return readme_content\n",
    "    else:\n",
    "        print(f\"Failed to fetch README for {repo_full_name}: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def check_for_arxiv_links(readme_content):\n",
    "    m = ARXIV_REGEX.search(readme_content)\n",
    "    if m:\n",
    "        url = m.group(0)\n",
    "        return url\n",
    "\n",
    "def d(repo_list, token=None):\n",
    "    xs = []\n",
    "    for repo in repo_list:\n",
    "        readme_content = fetch_readme(repo, token)\n",
    "        if readme_content:\n",
    "            x = check_for_arxiv_links(readme_content)\n",
    "            if x:\n",
    "                print(x)\n",
    "                xs.append(x)\n",
    "            else:\n",
    "                print(None)\n",
    "    return xs\n",
    "\n",
    "github_token = os.environ.get(\"GH_PAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"/\".join(list(unique_repos)[0].split(\"/\")[3:])\n",
    "\n",
    "mlbench_repos = [\"/\".join(repo.split(\"/\")[3:]) for repo in list(unique_repos)]\n",
    "# mlbench_repos\n",
    "arxiv_links = d(mlbench_repos, github_token)\n",
    "# paperlist = d([\"/\".join(repo.split(\"/\")[3:]) for repo in list(unique_repos)], github_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arxiv_links[0].split(\"/\")[-1]\n",
    "arxiv_ids = [link.split(\"/\")[-1] for link in arxiv_links]\n",
    "arxiv_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# arxiv_search.ArxivScraper().download_papers(ids = arxiv_ids[:1], dirpath=\"./MLBench_papers/\")\n",
    "# arxiv_search.ArxivScraper().download_papers(ids = arxiv_ids[1:], dirpath=\"./MLBench_papers/\")\n",
    "for pid in arxiv_ids[2:]:\n",
    "    arxiv_search.ArxivScraper().download_papers(ids = [pid], dirpath=\"./MLBench_papers/\")\n",
    "    print(f\"Downloaded {pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_documents = emb.load_and_chunk_papers(\n",
    "    pdf_path=\"./MLBench_papers/\",\n",
    ")\n",
    "# print(len(chunked_documents))\n",
    "# chunked_documents[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null char with unicode replacement char\n",
    "for i in range(len(chunked_documents)):\n",
    "    chunked_documents[i].page_content = chunked_documents[i].page_content.replace(\n",
    "        \"\\x00\", \"\\uFFFD\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"MLBench_papers\"\n",
    "pgvec_connection = emb.create_embedding_collection(\n",
    "    chunked_docs=chunked_documents,\n",
    "    embeddings=emb.get_embedding_func(model=\"text-embedding-3-large\"),\n",
    "    collection_name=collection_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgvec_connection.similarity_search(\"kidney embryos\", k=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a2a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
