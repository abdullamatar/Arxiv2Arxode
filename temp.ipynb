{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxiv_paper import ArxivPaper\n",
    "from arxiv_search import ArxivScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'arxiv_paper.ArxivPaper'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArxivPaper(pid='2307.09793v1', title='On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models', authors=<class 'arxiv.arxiv.Result.Author'>, abstract='Since late 2022, Large Language Models (LLMs) have become very prominent with\\nLLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs\\nare announced each week, many of which are deposited to Hugging Face, a\\nrepository of machine learning models and datasets. To date, nearly 16,000 Text\\nGeneration models have been uploaded to the site. Given the huge influx of\\nLLMs, it is of interest to know which LLM backbones, settings, training\\nmethods, and families are popular or trending. However, there is no\\ncomprehensive index of LLMs available. We take advantage of the relatively\\nsystematic nomenclature of Hugging Face LLMs to perform hierarchical clustering\\nand identify communities amongst LLMs using n-grams and term frequency-inverse\\ndocument frequency. Our methods successfully identify families of LLMs and\\naccurately cluster LLMs into meaningful subgroups. We present a public web\\napplication to navigate and explore Constellation, our atlas of 15,821 LLMs.\\nConstellation rapidly generates a variety of visualizations, namely\\ndendrograms, graphs, word clouds, and scatter plots. Constellation is available\\nat the following link: https://constellation.sites.stanford.edu/.', link='http://arxiv.org/pdf/2307.09793v1'),\n",
       " ArxivPaper(pid='2308.08241v1', title=\"TEST: Text Prototype Aligned Embedding to Activate LLM's Ability for Time Series\", authors=<class 'arxiv.arxiv.Result.Author'>, abstract=\"This work summarizes two strategies for completing time-series (TS) tasks\\nusing today's language model (LLM): LLM-for-TS, design and train a fundamental\\nlarge model for TS data; TS-for-LLM, enable the pre-trained LLM to handle TS\\ndata. Considering the insufficient data accumulation, limited resources, and\\nsemantic context requirements, this work focuses on TS-for-LLM methods, where\\nwe aim to activate LLM's ability for TS data by designing a TS embedding method\\nsuitable for LLM. The proposed method is named TEST. It first tokenizes TS,\\nbuilds an encoder to embed them by instance-wise, feature-wise, and\\ntext-prototype-aligned contrast, and then creates prompts to make LLM more open\\nto embeddings, and finally implements TS tasks. Experiments are carried out on\\nTS classification and forecasting tasks using 8 LLMs with different structures\\nand sizes. Although its results cannot significantly outperform the current\\nSOTA models customized for TS tasks, by treating LLM as the pattern machine, it\\ncan endow LLM's ability to process TS data without compromising the language\\nability. This paper is intended to serve as a foundational work that will\\ninspire further research.\", link='http://arxiv.org/pdf/2308.08241v1'),\n",
       " ArxivPaper(pid='2306.05212v1', title='RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit', authors=<class 'arxiv.arxiv.Result.Author'>, abstract='Although Large Language Models (LLMs) have demonstrated extraordinary\\ncapabilities in many domains, they still have a tendency to hallucinate and\\ngenerate fictitious responses to user requests. This problem can be alleviated\\nby augmenting LLMs with information retrieval (IR) systems (also known as\\nretrieval-augmented LLMs). Applying this strategy, LLMs can generate more\\nfactual texts in response to user input according to the relevant content\\nretrieved by IR systems from external corpora as references. In addition, by\\nincorporating external knowledge, retrieval-augmented LLMs can answer in-domain\\nquestions that cannot be answered by solely relying on the world knowledge\\nstored in parameters. To support research in this area and facilitate the\\ndevelopment of retrieval-augmented LLM systems, we develop RETA-LLM, a\\n{RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline\\nto help researchers and users build their customized in-domain LLM-based\\nsystems. Compared with previous retrieval-augmented LLM systems, RETA-LLM\\nprovides more plug-and-play modules to support better interaction between IR\\nsystems and LLMs, including {request rewriting, document retrieval, passage\\nextraction, answer generation, and fact checking} modules. Our toolkit is\\npublicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.', link='http://arxiv.org/pdf/2306.05212v1'),\n",
       " ArxivPaper(pid='2305.12720v1', title='llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large Language Models and its Methodology', authors=<class 'arxiv.arxiv.Result.Author'>, abstract='This study constructed a Japanese chat dataset for tuning large language\\nmodels (LLMs), which consist of about 8.4 million records. Recently, LLMs have\\nbeen developed and gaining popularity. However, high-performing LLMs are\\nusually mainly for English. There are two ways to support languages other than\\nEnglish by those LLMs: constructing LLMs from scratch or tuning existing\\nmodels. However, in both ways, datasets are necessary parts. In this study, we\\nfocused on supporting Japanese in those LLMs and making a dataset for training\\nor tuning LLMs in Japanese. The dataset we constructed consisted of various\\ntasks, such as translation and knowledge tasks. In our experiment, we tuned an\\nexisting LLM using our dataset and evaluated the performance qualitatively. The\\nresults suggest that our dataset is possibly beneficial for LLMs. However, we\\nalso revealed some difficulties in constructing LLMs in languages other than\\nEnglish.', link='http://arxiv.org/pdf/2305.12720v1'),\n",
       " ArxivPaper(pid='2309.14348v1', title='Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM', authors=<class 'arxiv.arxiv.Result.Author'>, abstract='Recently, Large Language Models (LLMs) have made significant advancements and\\nare now widely used across various domains. Unfortunately, there has been a\\nrising concern that LLMs can be misused to generate harmful or malicious\\ncontent. Though a line of research has focused on aligning LLMs with human\\nvalues and preventing them from producing inappropriate content, such\\nalignments are usually vulnerable and can be bypassed by alignment-breaking\\nattacks via adversarially optimized or handcrafted jailbreaking prompts. In\\nthis work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against\\npotential alignment-breaking attacks. RA-LLM can be directly constructed upon\\nan existing aligned LLM with a robust alignment checking function, without\\nrequiring any expensive retraining or fine-tuning process of the original LLM.\\nFurthermore, we also provide a theoretical analysis for RA-LLM to verify its\\neffectiveness in defending against alignment-breaking attacks. Through\\nreal-world experiments on open-source large language models, we demonstrate\\nthat RA-LLM can successfully defend against both state-of-the-art adversarial\\nprompts and popular handcrafted jailbreaking prompts by reducing their attack\\nsuccess rates from nearly 100\\\\% to around 10\\\\% or less.', link='http://arxiv.org/pdf/2309.14348v1'),\n",
       " ArxivPaper(pid='2309.17179v1', title='Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training', authors=<class 'arxiv.arxiv.Result.Author'>, abstract=\"Large language models (LLMs) typically employ sampling or beam search,\\naccompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and\\ndecoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via\\nPlanning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing\\ntree-search algorithms to guide multi-step reasoning. These methods mainly\\nfocus on LLMs' reasoning ability during inference and heavily rely on\\nhuman-designed prompts to activate LLM as a value function, which lacks general\\napplicability and scalability. To address these limitations, we present an\\nAlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically\\nillustrating how tree-search with a learned value function can guide LLMs'\\ndecoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a\\nlearned value function, our approach can be generally applied to different\\ntasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without\\nprompting advanced, large-scale models. (2) It can guide LLM's decoding during\\nboth inference and training. Empirical evaluations across reasoning, planning,\\nand RLHF alignment tasks validate the effectiveness of TS-LLM, even on trees\\nwith a depth of 64.\", link='http://arxiv.org/pdf/2309.17179v1'),\n",
       " ArxivPaper(pid='2310.16343v1', title='A Comprehensive Evaluation of Constrained Text Generation for Large Language Models', authors=<class 'arxiv.arxiv.Result.Author'>, abstract=\"Advancements in natural language generation (NLG) and large language models\\n(LLMs) have led to proficient text generation in various tasks. However,\\nintegrating intricate constraints into neural text generation, due to LLMs'\\nopacity, remains challenging. This study investigates constrained text\\ngeneration for LLMs, where predefined constraints are applied during LLM's\\ngeneration process. Our research examines multiple LLMs, including ChatGPT and\\nGPT-4, categorizing constraints into lexical, structural, and relation-based\\ntypes. We also present various benchmarks to facilitate fair evaluation. The\\nstudy addresses some key research questions, including the extent of LLMs'\\ncompliance with constraints. Results illuminate LLMs' capacity and deficiency\\nto incorporate constraints and provide insights for future developments in\\nconstrained text generation. Codes and datasets will be released upon\\nacceptance.\", link='http://arxiv.org/pdf/2310.16343v1'),\n",
       " ArxivPaper(pid='2309.16289v1', title='LawBench: Benchmarking Legal Knowledge of Large Language Models', authors=<class 'arxiv.arxiv.Result.Author'>, abstract=\"Large language models (LLMs) have demonstrated strong capabilities in various\\naspects. However, when applying them to the highly specialized, safe-critical\\nlegal domain, it is unclear how much legal knowledge they possess and whether\\nthey can reliably perform legal-related tasks. To address this gap, we propose\\na comprehensive evaluation benchmark LawBench. LawBench has been meticulously\\ncrafted to have precise assessment of the LLMs' legal capabilities from three\\ncognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize\\nneeded legal concepts, articles and facts; (2) Legal knowledge understanding:\\nwhether LLMs can comprehend entities, events and relationships within legal\\ntext; (3) Legal knowledge applying: whether LLMs can properly utilize their\\nlegal knowledge and make necessary reasoning steps to solve realistic legal\\ntasks. LawBench contains 20 diverse tasks covering 5 task types: single-label\\nclassification (SLC), multi-label classification (MLC), regression, extraction\\nand generation. We perform extensive evaluations of 51 LLMs on LawBench,\\nincluding 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific\\nLLMs. The results show that GPT-4 remains the best-performing LLM in the legal\\ndomain, surpassing the others by a significant margin. While fine-tuning LLMs\\non legal specific text brings certain improvements, we are still a long way\\nfrom obtaining usable and reliable LLMs in legal tasks. All data, model\\npredictions and evaluation code are released in\\nhttps://github.com/open-compass/LawBench/. We hope this benchmark provides\\nin-depth understanding of the LLMs' domain-specified capabilities and speed up\\nthe development of LLMs in the legal domain.\", link='http://arxiv.org/pdf/2309.16289v1'),\n",
       " ArxivPaper(pid='2305.05176v1', title='FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance', authors=<class 'arxiv.arxiv.Result.Author'>, abstract='There is a rapidly growing number of large language models (LLMs) that users\\ncan query for a fee. We review the cost associated with querying popular LLM\\nAPIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have\\nheterogeneous pricing structures, with fees that can differ by two orders of\\nmagnitude. In particular, using LLMs on large collections of queries and text\\ncan be expensive. Motivated by this, we outline and discuss three types of\\nstrategies that users can exploit to reduce the inference cost associated with\\nusing LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As\\nan example, we propose FrugalGPT, a simple yet flexible instantiation of LLM\\ncascade which learns which combinations of LLMs to use for different queries in\\norder to reduce cost and improve accuracy. Our experiments show that FrugalGPT\\ncan match the performance of the best individual LLM (e.g. GPT-4) with up to\\n98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost.\\nThe ideas and findings presented here lay a foundation for using LLMs\\nsustainably and efficiently.', link='http://arxiv.org/pdf/2305.05176v1'),\n",
       " ArxivPaper(pid='2307.07705v1', title='CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models', authors=<class 'arxiv.arxiv.Result.Author'>, abstract='Parameter-efficient tuning (PET) has been widely explored in recent years\\nbecause it tunes much fewer parameters (PET modules) than full-parameter\\nfine-tuning (FT) while still stimulating sufficient knowledge from large\\nlanguage models (LLMs) for downstream tasks. Moreover, when PET is employed to\\nserve multiple tasks, different task-specific PET modules can be built on a\\nfrozen LLM, avoiding redundant LLM deployments. Although PET significantly\\nreduces the cost of tuning and deploying LLMs, its inference still suffers from\\nthe computational bottleneck of LLMs. To address the above issue, we propose an\\neffective PET framework based on compressed LLMs, named \"CPET\". In CPET, we\\nevaluate the impact of mainstream LLM compression techniques on PET performance\\nand then introduce knowledge inheritance and recovery strategies to restore the\\nknowledge loss caused by these compression techniques. Our experimental results\\ndemonstrate that, owing to the restoring strategies of CPET, collaborating\\ntask-specific PET modules with a compressed LLM can achieve comparable\\nperformance to collaborating PET modules with the original version of the\\ncompressed LLM and outperform directly applying vanilla PET methods to the\\ncompressed LLM.', link='http://arxiv.org/pdf/2307.07705v1')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ◉_◉\n",
    "axs = ArxivScraper()\n",
    "papers = axs.search_papers('all:LLM', max_results=10)\n",
    "print(type(papers[0]))\n",
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs.download_paper(papers, dirpath='temp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv2arxode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
