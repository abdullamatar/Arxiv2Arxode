{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ◉_◉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc, score in query_response:\n",
    "#     print(\"*\" * 40)\n",
    "#     print(doc.dict().keys())\n",
    "#     # print(doc.from_orm(doc).dict())\n",
    "#     print(\"score: %s\" % score)\n",
    "#     print(doc.page_content)\n",
    "#     print(\"*\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Set logging for the queries\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Clean PDFS, is everything relevant?\n",
    "query = \"\"\"What are some key take aways from the agent tuning to get me started? Lets go through it step by step, and help me devise a plan for implementing some of the ideas in python code.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.44,openai_api_key=oai_sk)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=db_conn.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs[0].dict().keys()\n",
    "unique_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proompt = \"\"\"Use the following bits of information and internalize them. Because, you will then be applying the knowledge in a step by step manner to help a research agent in formulating a plan to implement the ideas in python code. Remember to be concise and pragmatic in your answer, and also focus on changing the most specific thing for the most specific reason. Remember, this means a lot to us, experimentation and research are important and you are good at both.\n",
    "\n",
    "Here is the relevant information:\n",
    "{env_context}\n",
    "\n",
    "And this is the current task at hand:\n",
    "{task_description}\n",
    "\n",
    "Your answer:\"\"\"\n",
    "\n",
    "big_proompt = PromptTemplate(\n",
    "    template=proompt, input_variables=[\"env_context\", \"task_description\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.predict(\n",
    "    text=big_proompt.format_prompt(env_context=unique_docs, task_description=query).text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import extract_functions_from_repo\n",
    "from pathlib import Path\n",
    "codedir = Path.cwd() / \"temprepo\"\n",
    "all_functions = extract_functions_from_repo(codedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import create_embedding_collection, load_and_chunk_code, get_embedding_func\n",
    "import openai\n",
    "import langchain\n",
    "# db_conn = get_db_connection(\"agentTuning_code_vecdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_code_docs = load_and_chunk_code(\"./temprepo/\")\n",
    "split_code_docs\n",
    "code_embedding_dbconn = create_embedding_collection(\n",
    "    chunked_docs=split_code_docs, embeddings=get_embedding_func(),\n",
    "    collection_name=\"agentTuning_code_vecdb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in code_embedding_dbconn.similarity_search(\"mmlu eval\"):\n",
    "    print(doc.page_content)\n",
    "# print(code_embedding_dbconn.similarity_search(\"mmlu eval\")[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv2arxode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
