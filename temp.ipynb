{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxiv_search import ArxivScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ◉_◉\n",
    "axs = ArxivScraper()\n",
    "papers = axs.search_papers('all:LLM OR LLM AND Agent OR Agents', max_results=10)\n",
    "print(type(papers[0]))\n",
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "papers[0]\n",
    "# axs.download_papers(papers[:1], dirpath='temp')\n",
    "# axs.download_paper(papers, dirpath='temp')\n",
    "# for p in papers:\n",
    "#     if p.has_github_link():\n",
    "#         print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path ./temp/\n"
     ]
    }
   ],
   "source": [
    "from pre_proc_pdfs import init_vectorDB, create_word_embeddings, load_and_chunk_papers\n",
    "\n",
    "docs = load_and_chunk_papers('./temp/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = create_word_embeddings()\n",
    "\n",
    "db_conn = init_vectorDB(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, tuple, langchain.schema.document.Document, float)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_response =db_conn.similarity_search_with_score(\"How many publicly available language models are there?\")\n",
    "len(query_response), type(query_response[0]), type(query_response[0][0]), type(query_response[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "dict_keys(['page_content', 'metadata', 'type'])\n",
      "score: 0.15521328048822958\n",
      "Since late 2022, Large Language Models (LLMs) have become very prominent with LLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs are announced each week, many of which are deposited to Hugging Face, a repository of machine learning models and datasets. To date, nearly 16,000 Text Generation models have been uploaded to the site. Given the huge influx of LLMs, it is of interest to know which LLM backbones, settings, training methods, and families are popular or trending. However, there is no comprehensive index of LLMs available. We take advantage of the relatively systematic nomenclature to perform hierarchical clustering and identify of Hugging Face LLMs communities amongst LLMs using n-grams and term frequency-inverse document frequency. Our methods successfully identify families of LLMs and accurately cluster LLMs into meaningful subgroups. We present a public web application to navigate and explore Constellation, our atlas of 15,821 LLMs. Constellation\n",
      "****************************************\n",
      "****************************************\n",
      "dict_keys(['page_content', 'metadata', 'type'])\n",
      "score: 0.16293333018620848\n",
      "Introduction\n",
      "\n",
      "Large language models (LLMs) are trained to generate realistic text given a user prompt [1]. Popular LLMs include ChatGPT, Bard, and the LLaMa family of models [2]. In addition to large companies like OpenAI and Google, smaller research groups and individuals can also train LLMs and share them through Hugging Face, a popular machine learning repository [3,4]. As of July 18, 2023 at 12 PM (GMT -5), 15,821 LLMs (or at least, Text Generation models) were available publicly on Hugging Face. To our knowledge, few attempts have been made to organize these LLMs, perhaps due to the immense number of models. Inspired by the bioinformatics technique of using hierarchical clustering on DNA sequences, we apply hierarchical clustering to the Hugging Face model names, assuming that similar names indicate similarity [5]. We also construct a graph of LLMs and detect communities using the Louvain method. Additionally, we generate other visualizations and explore the data.\n",
      "\n",
      "3\n",
      "\n",
      "Methods\n",
      "****************************************\n",
      "****************************************\n",
      "dict_keys(['page_content', 'metadata', 'type'])\n",
      "score: 0.1718139401618729\n",
      "1\n",
      "\n",
      "On the Origin of LLMs: An Evolutionary Tree and Graph for 15,821 Large Language Models\n",
      "\n",
      "Sarah R Gao, Andrew K Gao Canyon Crest Academy, Stanford University\n",
      "\n",
      "2\n",
      "\n",
      "Abstract\n",
      "****************************************\n",
      "****************************************\n",
      "dict_keys(['page_content', 'metadata', 'type'])\n",
      "score: 0.17722437653250978\n",
      "By making Constellation publicly available, we hope to encourage more systematic and informed engagement with LLMs. As the landscape of LLMs continues to evolve rapidly, tools\n",
      "\n",
      "10\n",
      "\n",
      "such as Constellation will be instrumental in assisting the researcher and developer communities in keeping pace with these developments.\n",
      "\n",
      "References\n",
      "\n",
      "1. Gao, A. (2023, July 8). Prompt Engineering for Large Language Models. SSRN; SSRN.\n",
      "\n",
      "https://doi.org/10.2139/ssrn.4504303\n",
      "\n",
      "2. Arancio, J. (2023, April 17). Llama, Alpaca and Vicuna: the new Chatgpt running on your laptop. Medium.\n",
      "\n",
      "https://medium.com/@jeremyarancio/exploring-llamas-family-models-how-we-achieved-running-llms-on-l\n",
      "\n",
      "aptops-16bf2539a1bb\n",
      "\n",
      "3. Hiter, S. (2023, June 6). What Is a Large Language Model? | Guide to LLMs. EWEEK.\n",
      "\n",
      "https://www.eweek.com/artificial-intelligence/large-language-model/\n",
      "\n",
      "4. Hugging Face. (n.d.). Hugging Face – On a mission to solve NLP, one commit at a time. Huggingface.co.\n",
      "\n",
      "https://huggingface.co/\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "for doc, score in query_response:\n",
    "    print(\"*\" * 40)\n",
    "    print(doc.dict().keys())\n",
    "    # print(doc.from_orm(doc).dict())\n",
    "    print(\"score: %s\" % score)\n",
    "    print(doc.page_content)\n",
    "    print(\"*\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reconnect to a vec_db, just call db_conn = PGVectorDB(collectioname, connstr, embeddingfunc)\n",
    "# from langchain.vectorstores.pgvector import PGVector\n",
    "# db_conn = PGVector('papers', 'postgresql://postgres:postgres@localhost:5432/postgres', embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv2arxode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
