{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Science Project Demonstration:\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql+psycopg2://llm_booster:langmod@localhost:5432/init_vectordb\n"
     ]
    }
   ],
   "source": [
    "import lib.embeddings as emb\n",
    "import utils.arxiv.arxiv_search as axs\n",
    "\n",
    "# arx_srch = axs.ArxivScraper()\n",
    "# papers = arx_srch.search_papers(\"Graph Neural Networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"type of papers: {type(papers[0])}\\navailable attributes for scraper: {dir(arx_srch)[-2:]}\"\n",
    ")\n",
    "print(f\"len of paper: {len(papers)}\")\n",
    "papers[0]\n",
    "# Downloaded just 1 paper, feel free to change...\n",
    "arx_srch.download_papers(papers=papers[:1], dirpath=\"./temp_papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_and_titles = {p.title: p.link for p in papers}\n",
    "links_and_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single file for the sake of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Guiding Pretraining in Reinforcement Learning with Large Language Models\\nYuqing Du* 1Olivia Watkins* 1Zihan Wang2C´edric Colas3 4Trevor Darrell1Pieter Abbeel1\\nAbhishek Gupta2Jacob Andreas3\\nAbstract\\nReinforcement learning algorithms typically\\nstruggle in the absence of a dense, well-shaped\\nreward function. Intrinsically motivated explo-\\nration methods address this limitation by reward-\\ning agents for visiting novel states or transitions,\\nbut these methods offer limited benefits in large\\nenvironments where most discovered novelty is\\nirrelevant for downstream tasks. We describe a\\nmethod that uses background knowledge from\\ntext corpora to shape exploration. This method,\\ncalled ELLM ( Exploring with LLM s) rewards an\\nagent for achieving goals suggested by a language\\nmodel prompted with a description of the agent’s\\ncurrent state. By leveraging large-scale language\\nmodel pretraining, ELLM guides agents toward\\nhuman-meaningful and plausibly useful behav-', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 0}),\n",
       " Document(page_content='iors without requiring a human in the loop. We\\nevaluate ELLM in the Crafter game environment\\nand the Housekeep robotic simulator, showing\\nthat ELLM-trained agents have better coverage\\nof common-sense behaviors during pretraining\\nand usually match or improve performance on\\na range of downstream tasks. Code available at\\nhttps://github.com/yuqingd/ellm .\\n1. Introduction\\nReinforcement learning algorithms work well when learners\\nreceive frequent rewards that incentivize progress toward\\ntarget behaviors. But hand-defining such reward functions\\nrequires significant engineering efforts in all but the simplest\\ncases (Amodei et al., 2016; Lehman et al., 2020). To master\\n*Equal contribution1Department of Electrical Engineer-\\ning and Computer Science, University of California, Berke-\\nley, USA2University of Washington, Seattle3Massachusetts In-\\nstitute of Technology, Computer Science and Artificial Intelli-\\ngence Laboratory4Inria, Flowers Laboratory. Correspondence to:', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 0}),\n",
       " Document(page_content='Yuqing Du <yuqing du@berkeley.edu >, Olivia Watkins <olivi-\\nawatkins@berkeley.edu >.\\nProceedings of the 40thInternational Conference on Machine\\nLearning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\\n2023 by the author(s).\\n1. Cut down the tree. \\n2. Craft a pickaxe. \\n3. Eat cow. \\n4. Sleep. \\n   . . . \\nk. Build a wood house. You see trees, \\ncows, grass, \\ntable, and \\nbushes. You have \\nwood in your \\ninventory. You \\nfeel hungry, \\nthirsty, and \\nsleepy. LLM \\nPrompt: \\nWhat should \\nyou do next? \\nFigure 1: ELLM uses a pretrained large language model\\n(LLM) to suggest plausibly useful goals in a task-agnostic\\nway. Building on LLM capabilities such as context-\\nsensitivity and common-sense, ELLM trains RL agents to\\npursue goals that are likely meaningful without requiring\\ndirect human intervention. Prompt is illustrative; see full\\nprompt and goal format in Appendix D.\\ncomplex tasks in practice, RL agents may therefore need to\\nlearn some behaviors in the absence of externally-defined', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 0}),\n",
       " Document(page_content='rewards. What should they learn?\\nIntrinsically motivated RL methods answer this question\\nby augmenting rewards with auxiliary objectives based on\\nnovelty, surprise, uncertainty, or prediction errors (Belle-\\nmare et al., 2016; Pathak et al., 2017; Burda et al., 2019;\\nZhang et al., 2021; Liu & Abbeel, 2021; Yarats et al., 2021).\\nBut not everything novel or unpredictable is useful: noisy\\nTVs and the movements of leaves on a tree may provide an\\ninfinite amount of novelty, but do not lead to meaningful\\nbehaviors (Burda et al., 2019). More recent approaches com-\\npute novelty with higher-level representations like language\\n(Tam et al., 2022; Mu et al., 2022), but can continue driving\\nthe agent to explore behaviors that are unlikely to corre-\\nspond to any human-meaningful goal—like enumerating\\nunique configurations of furniture in a household. It is not\\nsufficient for extrinsic-reward-free RL agents to optimize\\nfor novelty alone: learned behaviors must also be useful.', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 0}),\n",
       " Document(page_content='In this paper, we describe a method for using not just\\nlanguage-based representations but pretrained language\\nmodels (LLMs) as a source of information about useful\\nbehavior. LLMs are probabilistic models of text trained on\\nlarge text corpora; their predictions encode rich information\\n1arXiv:2302.06692v2  [cs.LG]  15 Sep 2023', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 0}),\n",
       " Document(page_content='Guiding Pretraining in Reinforcement Learning with Large Language Models\\nabout human common-sense knowledge and cultural conven-\\ntions. Our method, Exploring with LLM s (ELLM), queries\\nLMs for possible goals given an agent’s current context and\\nrewards agents for accomplishing those suggestions. As a\\nresult, exploration is biased towards completion of goals\\nthat are diverse, context-sensitive, and human-meaningful.\\nELLM-trained agents exhibit better coverage of useful be-\\nhaviors during pretraining, and outperform or match base-\\nlines when fine-tuned on downstream tasks.\\n2. Background and Related Work\\nIntrinsically Motivated RL. When reward functions are\\nsparse, agents often need to carry out a long, specific se-\\nquence of actions to achieve target tasks. As action spaces\\nor target behaviors grow more complex, the space of alter-\\nnative action sequences agents can explore grows combi-\\nnatorially. In such scenarios, undirected exploration that', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 1}),\n",
       " Document(page_content='randomly perturbs actions or policy parameters has little\\nchance of succeeding (Ten et al., 2022; Ladosz et al., 2022).\\nMany distinct action sequences can lead to similar out-\\ncomes (Baranes & Oudeyer, 2013)—for example, most\\naction sequences cause a humanoid agent to fall, while\\nvery few make it walk. Building on this observation, in-\\ntrinsically motivated RL algorithms (IM-RL) choose to\\nexplore outcomes rather than actions (Oudeyer & Kaplan,\\n2009; Ten et al., 2022; Ladosz et al., 2022). Knowledge-\\nbased IMs (KB-IMs) focus on maximising the diversity of\\nstates (reviews in Aubret et al., 2019; Linke et al., 2020).\\nCompetence-based IMs (CB-IMs) maximise the diversity\\nofskills mastered by the agent (review in Colas et al., 2022).\\nBecause most action sequences lead to a very restricted part\\nof the outcome space (e.g. all different ways of falling on\\nthe floor likely correspond to a single outcome), these meth-\\nods lead to a greater diversity of outcomes than undirected', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 1}),\n",
       " Document(page_content='exploration (Lehman et al., 2008; Colas et al., 2018).\\nHowever, maximizing diversity of outcomes may not always\\nbe enough. Complex environments can contain sources of\\ninfinite novelty. In such environments, seeking ever-more-\\nnovel states might drive learning towards behaviors that\\nhave little relevance to the true task reward. Humans do\\nnot explore outcome spaces uniformly, but instead rely on\\ntheir physical and social common-sense to explore plausibly-\\nuseful behaviors first. In video games, they know that keys\\nshould be used to open doors, ladders should be climbed,\\nand snakes might be enemies. If this semantic information\\nis removed, their exploration becomes severely impacted\\n(Dubey et al., 2018). The approach we introduce in this\\npaper, ELLM, may be interpreted as a CB-IM algorithm\\nthat seeks to explore the space of possible and plausibly-\\nuseful skills informed by human prior knowledge.Linguistic Goals and Pretrained Language Models.', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 1}),\n",
       " Document(page_content='One way of representing a diverse outcome space for ex-\\nploration is through language. Training agents to achieve\\nlanguage goals brings several advantages: (1) goals are easy\\nto express for non-expert users; (2) they can be more abstract\\nthan standard state-based goals (Colas et al., 2022); and (3)\\nagents can generalize better thanks to the partial composi-\\ntionality and recursivity of language (Hermann et al., 2017;\\nHill et al., 2019; Colas et al., 2020). Such linguistic goals\\ncan be used as instructions for language-conditioned imita-\\ntion learning or RL. In RL, agents typically receive language\\ninstructions corresponding to the relevant reward functions\\n(Luketina et al., 2019) and are only rarely intrinsically mo-\\ntivated (with the exception of Mu et al., 2022; Colas et al.,\\n2020; Tam et al., 2022), where language is also used as a\\nmore general compact state abstraction for task-agnostic\\nexploration.\\nRepresenting goals in language unlocks the possibility of us-', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 1}),\n",
       " Document(page_content='ing text representations and generative models of text (large\\nlanguage models, or LLMs) trained on large corpora. In im-\\nitation learning, text pretraining can help learners automat-\\nically recognize sub-goals and learn modular sub-policies\\nfrom unlabelled demonstrations (Lynch & Sermanet, 2020;\\nSharma et al., 2021), or chain pre-trained goal-oriented poli-\\ncies together to accomplish high-level tasks (Yao et al., 2020;\\nHuang et al., 2022a; Ahn et al., 2022; Huang et al., 2022b).\\nIn RL, LM-encoded goal descriptions greatly improve the\\ngeneralization of instruction-following agents across instruc-\\ntions (Chan et al., 2019) and from synthetic to natural goals\\n(Hill et al., 2020). LLMs have also been used as proxy\\nreward functions when prompted with desired behaviors\\n(Kwon et al., 2023). Unlike these approaches, ELLM uses\\npretrained LLMs to constrain exploration towards plausibly-\\nuseful goals in a task-agnostic manner. It does not assume a', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_documents = emb.load_and_chunk_papers(\n",
    "    pdf_path=\"temp_papers/guidingpretraining.pdf\",\n",
    "    single_file=True,\n",
    ")\n",
    "\n",
    "print(len(chunked_documents))\n",
    "chunked_documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(chunked_documents)):\n",
    "    chunked_documents[i].page_content = chunked_documents[i].page_content.replace(\n",
    "        \"\\x00\", \"\\uFFFD\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"eval4_db_guidingpretraining\"\n",
    "pgvec_connection = emb.create_embedding_collection(\n",
    "    chunked_docs=chunked_documents,\n",
    "    embeddings=emb.get_embedding_func(),\n",
    "    collection_name=collection_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgvec_connection = emb.get_db_connection(collection_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='state representations. ELLM may thus serve as a platform\\nfor future work that develops even more general and flexible\\nstrategies for incorporating human background knowledge\\ninto reinforcement learning.\\n6. Acknowledgements\\nYD and OW are funded by the Center for Human-\\nCompatible Artificial Intelligence. CC received funding\\nfrom the European Union’s Horizon 2020 research and in-\\nnovation programme under the Marie Skłodowska-Curie\\ngrant agreement No. 101065949. This material is based\\nupon work supported by the National Science Foundation\\nunder Grant No. 2212310 to AG and JA. OpenAI credits for\\nGPT-3 access were provided through OpenAI’s Researcher\\nAccess Program. We thank Sam Toyer and the members of\\nthe RLL for feedback on early iterations of this project.\\nReferences\\nAbid, A., Farooqi, M., and Zou, J. Persistent anti-muslim\\nbias in large language models. In Proceedings of the\\n2021 AAAI/ACM Conference on AI, Ethics, and Society ,pp. 298–306, 2021.', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 8}),\n",
       " Document(page_content='rect rearrangements for all possible objects, we investigate\\nwhether ELLM can be a general purpose method that guides\\nlearning human-meaningful behaviors.\\nUnlike Crafter’s combinatorial and high-level action space,\\nHousekeep operates with low-level actions: moving forward,\\nturning, looking up or down, and picking or placing an ob-\\nject. This allows us to investigate whether ELLM enables\\nhigh-level exploration despite using lower-level control. We\\nassume access to an egocentric instance segmentation sensor\\nto generate captions of in-view objects and receptacles, and\\nuse the text-davinci-002 InstructGPT model (Ouyang\\net al., 2022) as our LLM. Given a description of visible\\nobjects, the receptacles the objects are currently in, and all\\npreviously seen receptacles, we create a list of all possible\\nobject-receptacle mappings. We use the closed-form vari-\\nant of ELLM and query the LLM for whether each object\\nshould be placed in each receptacle as a yes/no question.', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 6}),\n",
       " Document(page_content='plausibly-useful exploratory goals satisfying the\\ndesiderata listed in Section 3.2: diversity, common-\\nsense and context sensitivity.\\n•(H2) Training an ELLM agent on these exploratory\\ngoals improves performance on downstream tasks com-\\npared to methods that do not leverage LLM-priors.\\nWe evaluate ELLM in two complex environments:\\n(1)Crafter , an open-ended environment in which explo-\\nration is required to discover long-term survival strategies\\n4', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 3}),\n",
       " Document(page_content='iors without requiring a human in the loop. We\\nevaluate ELLM in the Crafter game environment\\nand the Housekeep robotic simulator, showing\\nthat ELLM-trained agents have better coverage\\nof common-sense behaviors during pretraining\\nand usually match or improve performance on\\na range of downstream tasks. Code available at\\nhttps://github.com/yuqingd/ellm .\\n1. Introduction\\nReinforcement learning algorithms work well when learners\\nreceive frequent rewards that incentivize progress toward\\ntarget behaviors. But hand-defining such reward functions\\nrequires significant engineering efforts in all but the simplest\\ncases (Amodei et al., 2016; Lehman et al., 2020). To master\\n*Equal contribution1Department of Electrical Engineer-\\ning and Computer Science, University of California, Berke-\\nley, USA2University of Washington, Seattle3Massachusetts In-\\nstitute of Technology, Computer Science and Artificial Intelli-\\ngence Laboratory4Inria, Flowers Laboratory. Correspondence to:', metadata={'source': 'temp_papers/guidingpretraining.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgvec_connection.similarity_search(\"what is ELLM?\", k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into './rlsemiparam'...\n"
     ]
    }
   ],
   "source": [
    "from utils.code import clone_and_clean_repo\n",
    "\n",
    "clone_and_clean_repo(\n",
    "    git_url=\"https://github.com/OpenDFM/Rememberer\", target_dir=\"./rlsemiparam\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 7700.01it/s]\n"
     ]
    }
   ],
   "source": [
    "chunk_code = emb.load_and_chunk_code(code_path=\"./rlsemiparam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de4a4c5c-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4d56-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4d7e-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4d9c-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4da6-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4dba-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4dce-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4dec-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4e0a-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4e14-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4e28-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4e32-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4e46-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4e50-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4e64-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4e6e-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4e82-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4e96-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4ea0-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4eaa-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4ebe-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4ec8-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4edc-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4ee6-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4efa-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4f0e-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4f18-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4f2c-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4f36-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4fcc-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4fea-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a4ff4-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a5008-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a501c-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a5026-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a503a-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a5044-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a5058-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a5062-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a5076-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a5080-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a5094-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a509e-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a50b2-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a50bc-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a50d0-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a50da-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a50ee-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a50f8-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a510c-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a5116-ed05-11ee-ba5c-00155d75290d',\n",
       " 'de4a512a-ed05-11ee-ba5c-00155d75290d']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_code)\n",
    "pgvec_connection.add_documents(chunk_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Copyright 2023 SJTU X-Lance Lab\\n# \\n# Licensed under the Apache License, Version 2.0 (the \"License\");\\n# you may not use this file except in compliance with the License.\\n# You may obtain a copy of the License at\\n# \\n#     http://www.apache.org/licenses/LICENSE-2.0\\n# \\n# Unless required by applicable law or agreed to in writing, software\\n# distributed under the License is distributed on an \"AS IS\" BASIS,\\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\n# See the License for the specific language governing permissions and\\n# limitations under the License.\\n\\n# Created by Danyang Zhang @X-Lance.\\n\\nfrom typing import Dict, Tuple, Deque, List, Set\\nfrom typing import Union, Optional, Callable, Sequence, TypeVar, Generic, Hashable, Any\\nimport abc\\n#import dm_env\\n\\nimport numpy as np\\nimport collections\\nimport itertools\\nimport yaml\\nfrom sentence_transformers import SentenceTransformer\\nfrom sentence_transformers.util import dot_score\\nimport torch\\nimport copy\\n\\nimport logging\\n\\n#logger = logging.getLogger(\"agent.history\")\\nhlogger = logging.getLogger(\"history\")\\n\\nKey = TypeVar(\"Key\", bound=Hashable)\\nAction = TypeVar(\"Action\", bound=Hashable)\\n\\nclass Matcher(abc.ABC, Generic[Key]):\\n    #  class Matcher {{{ # \\n    def __init__(self, query: Key):\\n        #  method __init__ {{{ # \\n        self._query: Key = query\\n        #  }}} method __init__ # \\n\\n    def __call__(self, key: Key) -> float:\\n        raise NotImplementedError\\n    #  }}} class Matcher # \\n\\nMatcherConstructor = Callable[[Key], Matcher[Key]]\\n\\nclass LCSNodeMatcher(Matcher[Tuple[str, Any]]):\\n    #  class LCSNodeMatcher {{{ # \\n    def __init__(self, query: Tuple[str, ...]):\\n        #  method __init__ {{{ # \\n        super(LCSNodeMatcher, self).__init__(query)\\n\\n        screen: str = self._query[0]\\n        self._node_sequence: List[str] = list( map( lambda n: n[1:n.index(\" \")]\\n                                                  , screen.splitlines()\\n                                                  )\\n                                             )\\n        #  }}} method __init__ # \\n\\n    def __call__(self, key: Tuple[str, ...]) -> float:\\n        #  method __call__ {{{ # \\n        key_screen: str = key[0]\\n        key_node_sequence: List[str] = list( map( lambda n: n[1:n.index(\" \")]\\n                                                , key_screen.splitlines()\\n                                                )\\n                                           )\\n\\n        n: int = len(self._node_sequence)\\n        m: int = len(key_node_sequence)\\n        lcs_matrix: np.ndarray = np.zeros((n+1, m+1), dtype=np.int32)\\n        for i, j in itertools.product( range(1, n+1)\\n                                     , range(1, m+1)\\n                                     ):\\n            lcs_matrix[i, j] = lcs_matrix[i-1, j-1] + 1 if self._node_sequence[i-1]==key_node_sequence[j-1]\\\\\\n                                                        else max( lcs_matrix[i-1, j]\\n                                                                , lcs_matrix[i, j-1]\\n                                                                )\\n        lcs: np.int32 = lcs_matrix[n, m]\\n        length: int = max(n, m)\\n        similarity: float = float(lcs)/length\\n\\n        hlogger.debug(\"Req: %s\", \" \".join(self._node_sequence))\\n        hlogger.debug(\"Key: %s\", \" \".join(key_node_sequence))\\n        hlogger.debug( \"LCS: %d, L1: %d, L2: %d, Sim: %.2f\"\\n                    , lcs, n, m, similarity\\n                    )\\n\\n        return similarity\\n        #  }}} method __call__ # \\n    #  }}} class LCSNodeMatcher #', metadata={'source': '/home/sonofman/Research/Arxiv2Arxode/rlsemiparam/history.py'}),\n",
       " Document(page_content='def _insert_key( self, key: Key\\n                   , action_history: List[Action]\\n                   , last_reward: float\\n                   , total_reward: float\\n                   ) -> bool:\\n        #  method _insert_key {{{ # \\n\\n        hlogger.debug(\"Record: %d, Keys: %d\", len(self._record), len(self._keys))\\n\\n        if key not in self._record:\\n            #  Insertion Policy (Static Capacity Limie) {{{ # \\n            matcher: Matcher[Key] = self._matcher(key)\\n            similarities: np.ndarray = np.asarray(list(map(matcher, self._keys)))\\n\\n            if self._item_capacity is not None and self._item_capacity>0\\\\\\n                    and len(self._record)==self._item_capacity:\\n\\n                max_new_similarity_index: np.int64 = np.argmax(similarities)\\n                max_old_similarity_index: Tuple[ np.int64\\n                                               , np.int64\\n                                               ] = np.unravel_index( np.argmax(self._similarity_matrix)\\n                                                                   , self._similarity_matrix.shape\\n                                                                   )\\n                if similarities[max_new_similarity_index]>=self._similarity_matrix[max_old_similarity_index]:\\n                    # drop the new one\\n                    return False\\n                # drop an old one according to the number of action samples\\n                action_dict1: HistoryReplay.ActionDict = self._record[self._keys[max_old_similarity_index[0]]][\"action_dict\"]\\n                nb_samples1: int = sum(map(lambda d: d[\"number\"], action_dict1.values()))\\n\\n                action_dict2: HistoryReplay.ActionDict = self._record[self._keys[max_old_similarity_index[1]]][\"action_dict\"]\\n                nb_samples2: int = sum(map(lambda d: d[\"number\"], action_dict2.values()))\\n\\n                drop_index: np.int64 = max_old_similarity_index[0] if nb_samples1>=nb_samples2 else max_old_similarity_index[1]\\n\\n                del self._record[self._keys[drop_index]]\\n                self._keys[drop_index] = key\\n                similarities[drop_index] = 0.\\n                self._similarity_matrix[drop_index, :] = similarities\\n                self._similarity_matrix[:, drop_index] = similarities\\n                self._record[key] = { \"other_info\": { \"action_history\": action_history\\n                                                    , \"last_reward\": last_reward\\n                                                    , \"total_reward\": total_reward\\n                                                    , \"number\": 1\\n                                                    }\\n                                    , \"action_dict\": {}\\n                                    , \"id\": self._max_id\\n                                    }\\n                self._max_id += 1\\n            else:\\n                #new_index: int = len(self._record)\\n                self._keys.append(key)\\n                #self._similarity_matrix[new_index, :new_index] = similarities\\n                #self._similarity_matrix[:new_index, new_index] = similarities\\n                self._record[key] = { \"other_info\": { \"action_history\": action_history\\n                                                    , \"last_reward\": last_reward\\n                                                    , \"total_reward\": total_reward\\n                                                    , \"number\": 1\\n                                                    }\\n                                    , \"action_dict\": {}\\n                                    , \"id\": self._max_id\\n                                    }\\n                self._max_id += 1\\n            #  }}} Insertion Policy (Static Capacity Limie) # \\n        else:\\n            other_info: HistoryReplay.InfoDict = self._record[key][\"other_info\"]\\n\\n            _update_action_history( self._action_history_update_mode\\n                                  , other_info, action_history\\n                                  )', metadata={'source': '/home/sonofman/Research/Arxiv2Arxode/rlsemiparam/history.py'}),\n",
       " Document(page_content='computed from the step instructions. It is noticed that the instructions follow some patterns, thus,\\nwe inspect the instructions and categorize them into six types. Then a similarity lookup table is\\ndesigned according to the instruction types. The details should be referred to in the supplementary.\\nThe observation similarity fois computed based on the length of the longest common sequence of\\nthe HTML elements in the screen representation:\\nfo(sc1, sc2) =lcs(sc1, sc2)\\nmax{len(sc1), len(sc2)}. (8)\\nThe full-trajectory expanding is adopted, as most of the tasks will end in 5 steps as well.\\n4.2 Results on WebShop\\nREMEMBERER is applied to WebShop with 2-shot in-context learning. The experience memory\\nis initialized with four annotated experiences of the decision step from one trajectory. The agent\\nis trained for 3 epochs on a training set containing 10 different tasks outside the test sets used by', metadata={'source': './temp/semiparamRLagents.pdf', 'page': 6}),\n",
       " Document(page_content='keys = list(self._record.keys())\\n        similarity_matrix = np.zeros( (len(keys), len(keys))\\n                                    , dtype=np.float32\\n                                    )\\n        for i in range(len(keys)):\\n            similarity_matrix[i, :i] = similarity_matrix[:i, i]\\n\\n            matcher: Matcher[Key] = self._matcher(keys[i])\\n            similarity_matrix[i, i+1:] = np.asarray(\\n                                            list( map( matcher\\n                                                     , keys[i+1:]\\n                                                     )\\n                                                )\\n                                          )\\n\\n        if self._item_capacity is not None\\\\\\n                and self._item_capacity>0\\\\\\n                and len(keys)>self._item_capacity:\\n            hlogger.warning( \"Boosting the item capacity from %d to %d\"\\n                           , self._item_capacity, len(keys)\\n                           )\\n            self._item_capacity = len(keys)\\n            self._similarity_matrix = similarity_matrix\\n        #else:\\n            #self._similarity_matrix[:len(keys), :len(keys)] = similarity_matrix\\n\\n        action_size: int = max( map( lambda rcd: len(rcd[\"action_dict\"])\\n                                   , self._record.values()\\n                                   )\\n                              )\\n        if self._action_capacity is not None\\\\\\n                and self._action_capacity>0\\\\\\n                and action_size > self._action_capacity:\\n            hlogger.warning( \"Boosting the item capacity from %d to %d\"\\n                           , self._action_capacity, action_size\\n                           )\\n            self._action_capacity = action_size\\n        self._keys = keys\\n\\n        self._max_id = max( map( lambda rcd: rcd[\"id\"]\\n                               , self._record.values()\\n                               )\\n                          ) + 1\\n        #  }}} method load_yaml # \\n    def save_yaml(self, yaml_file: str):\\n        with open(yaml_file, \"w\") as f:\\n            yaml.dump(self._record, f, Dumper=yaml.Dumper)\\n    def __len__(self) -> int:\\n        return len(self._record)\\n    #  }}} class HistoryReplay #', metadata={'source': '/home/sonofman/Research/Arxiv2Arxode/rlsemiparam/history.py'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pgvec_connection.similarity_search(\"lcs_matrix[i-1, j-1] + 1 if self._node_sequence[i-1]==key_node_sequence[j-1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The coordinator is the main orchestration class responsible for running and monitoring the code generation groupchat.\n",
    "---\n",
    "- Some main functions to highlight:\n",
    "    - `marl` initializes four agents in this case each with a unique role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mmain_userproxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Create a simplified example of graph attention networks in python to help me grasp the context.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "list index out of range\n",
      "\u001b[33mcoding_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool Call (call_W0VvWr0ZXm7acW5aJzBPSKSu): retrieve_content *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"query\": \"simplified example of Graph Attention Networks in Python\"\n",
      "}\n",
      "\u001b[32m*********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION retrieve_content...\u001b[0m\n",
      "\u001b[33mmain_userproxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33mmain_userproxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool \"call_W0VvWr0ZXm7acW5aJzBPSKSu\" *****\u001b[0m\n",
      "simplified example of Graph Attention Networks in Python\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcode_reviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "Here is a simple example of Graph Attention Networks (GAT) implementation using PyTorch Geometric:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "from torch_geometric.nn import GATConv\n",
      "\n",
      "class GAT(torch.nn.Module):\n",
      "    def __init__(self, in_channels, out_channels):\n",
      "        super(GAT, self).__init__()\n",
      "        self.conv1 = GATConv(in_channels, 8, heads=8, dropout=0.6)\n",
      "        self.conv2 = GATConv(8*8, out_channels, heads=1, dropout=0.6)\n",
      "\n",
      "    def forward(self, data):\n",
      "        x, edge_index = data.x, data.edge_index\n",
      "\n",
      "        x = F.dropout(x, p=0.6, training=self.training)\n",
      "        x = self.conv1(x, edge_index)\n",
      "        x = F.elu(x)\n",
      "        x = F.dropout(x, p=0.6, training=self.training)\n",
      "        x = self.conv2(x, edge_index)\n",
      "\n",
      "        return F.log_softmax(x, dim=1)\n",
      "```\n",
      "\n",
      "In this example:\n",
      "1. We define a GAT model with two GATConv layers.\n",
      "2. In the forward function, we apply dropout and GATConv to the node features `x` and the edge indices `edge_index`.\n",
      "3. We apply an activation function (here `F.elu`) to the output of the first GATConv layer. We then apply the second GATConv layer to it.\n",
      "4. Finally, we apply the Softmax function to the output of the second GATConv layer to get the final output scores for each node. You can think of these scores as the probability of each node belonging to a certain class (in the case of node classification tasks). \n",
      "\n",
      "Please note PyTorch Geometric is not a standard library and you would need to install it using pip:\n",
      "\n",
      "```\n",
      "pip install torch_geometric\n",
      "```\n",
      "\n",
      "Let me know if you want to discuss any part of the code in detail.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mmain_userproxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Can you help me to know how the Graph Attention Network actually works? I am a bit confused on its mechanism.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mcoding_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 2, in <module>\n",
      "    from torch_geometric.nn import GATConv\n",
      "ModuleNotFoundError: No module named 'torch_geometric'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mmain_userproxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure, I'd be happy to explain it!\n",
      "\n",
      "In Graph Attention Networks (GAT), the key idea is to use self-attention mechanism to weigh the neighbors of a node when aggregating their features. The key steps in a GAT are:\n",
      "\n",
      "1. Linear Transformation: Each node feature vector is first transformed using a shared linear transformation represented by a weight matrix.\n",
      "\n",
      "2. Attention Coefficients: Attention coefficients are then computed for each node with its neighbors. These coefficients capture the importance of the neighbor nodes to the node and are computed using a shared attentional mechanism. The mechanism is basically a feed-forward neural network with LeakyReLU nonlinearity.\n",
      "\n",
      "3. Masking: To efficiently compute the attention coefficients in a vectorized manner, all possible node pairs' attention scores are calculated but only the neighbors' are selected by applying a mask.\n",
      "\n",
      "4. Normalization: The attention coefficients for each node and its neighbors are then normalized using the softmax function. \n",
      "\n",
      "5. Weighted Feature Aggregation: Each neighbor’s features are multiplied by their normalized attention coefficients and summed to obtain the final output features of the node.\n",
      "\n",
      "6. Multi-head Attention: Similar to the Transformer model, GAT also uses multi-head attention where the model has multiple sets of weight matrices and attention mechanisms (the heads). The output feature vectors from all heads are then concatenated to form the final output feature vector.\n",
      "\n",
      "This is how the basic graph attention mechanism in the GAT works. The process is applied in a layer-wise manner in GAT. \n",
      "\n",
      "Let me know if you have more queries around this.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcode_reviewer\u001b[0m (to chat_manager):\n",
      "\n",
      "EVALUATION SCORE=0.0\n",
      "\n",
      "The code hasn't been properly generated due to a ModuleNotFoundError, this suggests that the necessary library 'torch_geometric' isn't installed in the environment. I suggest making sure that all necessary dependencies and libraries are installed before trying to run the program.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcoding_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "Sorry for the earlier confusion. Here is an example of Graph Attention Networks implementation in Python using PyTorch. You would need to install the PyTorch Geometric library if you haven't done so:\n",
      "\n",
      "```python\n",
      "# Import necessary libraries\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "from torch_geometric.nn import GATConv\n",
      "\n",
      "# Create a class for the Graph Attention Network\n",
      "class GAT(torch.nn.Module):\n",
      "    def __init__(self, in_channels, out_channels):\n",
      "        super(GAT, self).__init__()  # Inherit methods from torch.nn.Module\n",
      "\n",
      "        # Define the first GAT convolution layer with 8 heads\n",
      "        self.conv1 = GATConv(in_channels, 8, heads=8, dropout=0.6)\n",
      "\n",
      "        # Define the second GAT convolution layer with 1 head\n",
      "        self.conv2 = GATConv(8*8, out_channels, heads=1, dropout=0.6)\n",
      "\n",
      "    # Define forward pass\n",
      "    def forward(self, data):\n",
      "        x, edge_index = data.x, data.edge_index\n",
      "\n",
      "        # Apply dropout to inputs, GAT layer, then non-linearity\n",
      "        x = F.dropout(x, p=0.6, training=self.training)\n",
      "        x = self.conv1(x, edge_index)\n",
      "        x = F.elu(x)\n",
      "\n",
      "        # Apply dropout to output of first layer, GAT layer, then log_softmax\n",
      "        x = F.dropout(x, p=0.6, training=self.training)\n",
      "        x = self.conv2(x, edge_index)\n",
      "\n",
      "        return F.log_softmax(x, dim=1)\n",
      "```\n",
      "\n",
      "Please ensure you have the correct environment and dependencies installed to run this program. If you have more queries regarding how Graph Attention Networks work, feel free to ask.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mmain_userproxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Sure! \n",
      "\n",
      "Graph Attention Networks (GATs) work with the primary goal to weigh neighbors' contribution to the new node embedding. Unlike other node embedding methods like Graph Convolutional Network (GCN) that handle all neighbors equally, GAT introduces the attention mechanism to provide weights to neighbors. \n",
      "\n",
      "The GAT works by computing a set of attention coefficients to describe the importance of node's state to another's. These coefficients are calculated with a shared self-attentional layer, applying a LeakyReLU activation function. \n",
      "\n",
      "Each node's updated features are then calculated by applying a weighted combination of the current features and features sent by its neighboring nodes. The weights of the combination are the attention coefficients.\n",
      "\n",
      "The GAT extends the mechanism across multiple attention heads, similar to the Transformer model in natural language processing. The key idea is to stabilize the learning process of self-attention while capturing different types of relationships in different subspaces.\n",
      "\n",
      "Let's retrieve more detailed information to help you understand better.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from agents.coordinator import Coordinator\n",
    "from agents.agent import marl\n",
    "\n",
    "crd = Coordinator(\n",
    "    team_name=\"test\",\n",
    "    agents=marl(collection_name=\"cs_demo\"),\n",
    ")\n",
    "\n",
    "crd.code_gen_group_chat(\n",
    "    \"Create a simplified example of graph attention networks in python to help me grasp the context.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bismillah",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
